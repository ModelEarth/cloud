{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNYDh2lRKAie"
      },
      "source": [
        "# **Run Models Colab**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxZiI7xcrT4B"
      },
      "source": [
        "# Our 6 Models include Random Forests and XGBoost\n",
        "\n",
        "Choose models to run at https://model.earth/realitystream/models  \n",
        "Documentation https://model.earth/realitystream  \n",
        "Backup resides in the: [RealityStream models folder](https://github.com/ModelEarth/realitystream/tree/main/models)  \n",
        "Notes on running locally and in the cloud reside in our [cloud repo](https://github.com/modelearth/cloud/).\n",
        "\n",
        "PARTIALLY DONE: Add files to a \"report\" folder. We send it's content to GitHub in the last step.\n",
        "\n",
        "TO DO: For a unified html report, as each model completes, update a dataset with the performance accuracy scores and other metrics for all the models, then send to a report.md file. Save in the left 'report' folder which we push to Github in last step.\n",
        "\n",
        "DONE: Include the time it took to run each model in report.md. - TARUN\n",
        "\n",
        "DONE: Generate features-importance reports for available models. - Bin(Melody)\n",
        "\n",
        "DONE: Performance metrics—including accuracy, ROC-AUC, G-Mean, best threshold, and classification reports—were aggregated into a modelResults dictionary using abbreviated keys. Top 10 feature importances for applicable models were included, and results were formatted into summary tables. - Yogesh Gajula\n",
        "\n",
        "DONE: Function to calculate and append Correlation values to Unified Aggregation Results and Visual chart with prefix's for the top 10 Feature importances. - Yogesh Gajula\n",
        "\n",
        "TO DO: Fix the error: name 'save_dir' is not defined. - Is this still occuring?  \n",
        "Occured previously for both rbf and xgboost. Maybe others.\n",
        "\n",
        "DONE Aashish: Used Pandas for integrated_df (became df) when save_training = False.  \n",
        "DONE Loren: Loaded parameters.yaml and saved locally for customization.  \n",
        "https://chatgpt.com/share/e4a2ee73-ab74-4551-9868-37b9b5b6b359  \n",
        "DONE Tarun: Allow save-training to be set in the parameters.yaml values. Default to false. Use dash instead of underscore in yaml.\n",
        "\n",
        "TO DO: Test that default target path for bee data works by deleting in left panel after pullin in parameters.yaml. Then test that panels 15 and 16 work.  \n",
        "if param.targets.path: # Override with value from yaml  \n",
        "    target_url = param.targets.path\n",
        "\n",
        "target_df got problems.\n",
        "\n",
        "TO DO: Pull 2-column target zip code UN topics directly from Google Data Commons based DCID target value in parameters.yaml\n",
        "\n",
        "DONE Ivy: In the same panel as each accuracy report, call a new function called displayModelHeader to display the model name (as a bold header) and the file paths for features and targets above the report.\n",
        "\n",
        "DONE Ivy: Show the parameter values below each path at the top of each accuracy report. So under the Feature path we'd have:  \n",
        "startyear: 2017, endyear: 2021, naics: [6], state: ME\n",
        "\n",
        "DONE Lily: Add support for multiple states. After running the third panel, you can edit the custom yaml on the right to set state: CT, ME, MA, NH, RI, VT.  Then add a loop that runs when there are multiple states. We'll add a file called parameters-new-england.yaml in the root of the RealityStream repo with the six states as features.states. Load here and add python to loop through the states.\n",
        "\n",
        "TO DO: Add more parameters.yaml files that pull features/targets and join on the county Fips column. Add a path parameter that pulls from \"all-years\" which are generated by our [Industry Features CoLab](https://colab.research.google.com/drive/1HJnuilyEFjBpZLrgxDa4S0diekwMeqnh?usp=sharing). All years on GitHub:  \n",
        "https://github.com/ModelEarth/community-timelines/tree/main/training/all-years\n",
        "(These were created by Ronan)\n",
        "\n",
        "DONE: Load blinks/parameters-blinks.yaml and use target.column to limit to y column\n",
        "\n",
        "SAVE FOR LATER: Dropdown in webpage to send parameters.yaml 1 of these 4 bee targets (years).  \n",
        "https://github.com/ModelEarth/bee-data/tree/main/targets\n",
        "\n",
        "Done: Avoid sorting incoming parameters.yaml alphabetically. Attempt using  OrderedDict is commented out is several places below. Comment out prior alphabetical technique - we can provide a bool to toggle to it if it provides better security when requests are submitted through webpages. - Soham\n",
        "\n",
        "DONE: Only import models requested by parameters.yaml. Move \"from sklearn\" imports to step after parameters are edited in textbox. - Tarun\n",
        "\n",
        "IN PROGRESS: Creating install for Flask application with Google Cloud Run cmds at [github.com/modelearth/cloud](https://github.com/modelearth/cloud)\n",
        "\n",
        "DONE: Send the params loaded from the default path to the widget diplay. - Prathyusha\n",
        "\n",
        "DONE: Create an object that holds the 5 sample parameters.yaml paths that are on the RealityStream main page. When choosing one, send the path and the yaml it points at to the textarea below the path select menu. - Prathyusha\n",
        "\n",
        "DONE: Parameter files displayed in select menu. Instead pull the select options from parameter-paths.csv - Prathyusha\n",
        "\n",
        "DONE: Deactivate the right-side display of the yaml values and have the editing occur in the widget textbox. - Melody\n",
        "\n",
        "TODO: Find a way to delete the existing files in the colab environment which interfers with the code when we re-run\n",
        "\n",
        "TODO: imblearn import for cuML\n",
        "\n",
        "\n",
        "DONE: Canopy Data modified for all years for all states - Savar\n",
        "\n",
        "TODO: Modify geoId to show names\n",
        "\n",
        "DONE: Show visualizations of the county data drops\n",
        "\n",
        "TODO: Need to figure out useful data/js based graphs -Savar\n",
        "\n",
        "TODO: Trials to predict future state wise canopies with SVM followed by Logistic Reg -Savar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HABqp9DCVQ1n"
      },
      "source": [
        " ⚠️ Please change your runtime type to T4 GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries and Intital Set-up"
      ],
      "metadata": {
        "id": "LKV4En8ueA1E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToYmR0DBMLxv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dab4cd64-a525-45f1-c40c-8ab8d847d7d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished: uninstall\n",
            "Finished: install\n"
          ]
        }
      ],
      "source": [
        "### Important: First change your runtime time to: T4 GPU under: Runtime > Change runtime type\n",
        "\n",
        "### Important: Click \"Restart session\" when it pops up. This is required for older version of numpy used with cuml for faster Nividia processing.\n",
        "\n",
        "verbose = False # True if you want full install logs; False to minimize output.\n",
        "\n",
        "import subprocess\n",
        "\n",
        "def run_cmd(cmd):\n",
        "    \"\"\"\n",
        "    Runs a shell command. If verbose, prints stdout and stderr (on failure).\n",
        "    Otherwise prints a simple 'Finished: <action>' message.\n",
        "    Automatically uses a shell for 'python -c' commands to preserve quoting.\n",
        "    \"\"\"\n",
        "    # Decide how to invoke subprocess based on need for shell (for python -c)\n",
        "    if cmd.startswith(\"python -c\"):\n",
        "        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
        "    else:\n",
        "        parts = cmd.split()\n",
        "        result = subprocess.run(parts, capture_output=True, text=True)\n",
        "\n",
        "    # Determine label for clean output\n",
        "    if cmd.startswith(\"pip uninstall\"):\n",
        "        label = \"uninstall\"\n",
        "    elif cmd.startswith(\"pip install\"):\n",
        "        label = \"install\"\n",
        "    elif \"import numpy\" in cmd:\n",
        "        label = \"NumPy version check\"\n",
        "    elif \"import cuml\" in cmd:\n",
        "        label = \"cuML import check\"\n",
        "    else:\n",
        "        parts = cmd.split()\n",
        "        label = parts[1] if len(parts) > 1 else cmd\n",
        "\n",
        "    if verbose:\n",
        "        # verbose: show full output\n",
        "        print(result.stdout)\n",
        "        if result.returncode != 0:\n",
        "            print(result.stderr)\n",
        "    else:\n",
        "        # clean: show only action label\n",
        "        print(f\"Finished: {label}\")\n",
        "\n",
        "# Uninstall conflicting or incompatible libraries\n",
        "run_cmd(\n",
        "    \"pip uninstall -y jax jaxlib tensorflow treescope pymc thinc flax optax chex \"\n",
        "    \"orbax-checkpoint dopamine-rl tensorflow-decision-forests tables spacy mlxtend fastai blosc2\"\n",
        ")\n",
        "\n",
        "# Reinstall exact versions compatible with RAPIDS 25.2\n",
        "run_cmd(\n",
        "    \"pip install numpy==1.24.4 scikit-learn==1.2.2 imbalanced-learn==0.11.0 --force-reinstall\"\n",
        ")\n",
        "\n",
        "# Install RAPIDS 25.2 packages — all GPU-accelerated and version-pinned for compatibility\n",
        "run_cmd(\n",
        "    \"pip install --extra-index-url=https://pypi.nvidia.com \"\n",
        "    \"cudf-cu12==25.2.* cuml-cu12==25.2.* dask-cudf-cu12==25.2.* dask-cuda==25.2.* \"\n",
        "    \"rapids-dask-dependency==25.2.* raft-dask-cu12==25.2.* \"\n",
        "    \"rmm-cu12==25.2.* librmm-cu12==25.2.* pylibcudf-cu12==25.2.* \"\n",
        "    \"libraft-cu12==25.2.* pylibraft-cu12==25.2.* libcuvs-cu12==25.2.* \"\n",
        "    \"cuvs-cu12==25.2.* ucx-py-cu12==0.42.* ucxx-cu12==0.42.* distributed-ucxx-cu12==0.42.*\"\n",
        ")\n",
        "\n",
        "# Confirm installations\n",
        "run_cmd(\n",
        "    'python -c \"import numpy; print(\\'NumPy version:\\', numpy.__version__)\"'\n",
        ")\n",
        "run_cmd(\n",
        "    'python -c \"import cuml; print(\\'cuML imported successfully\\')\"'\n",
        ")\n",
        "print('Click the  \"Restart Session\" button when it pops up.')\n",
        "print('To run all subsequent steps, click the next cell and choose \"Runtime > Run cell and below\"')\n",
        "\n",
        "from google.colab import runtime\n",
        "from IPython.display import display, HTML, Javascript\n",
        "\n",
        "# Create a button with JavaScript that directly uses Colab's runtime API\n",
        "# restart_button = HTML('''\n",
        "# <button id=\"restartButton\" style=\"background-color:#4CAF50; color:white; padding:8px 16px; border:none; border-radius:4px; cursor:pointer;\">\n",
        "#   Restart Session\n",
        "# </button>\n",
        "# <div id=\"statusMessage\" style=\"margin-top:8px;\"></div>\n",
        "\n",
        "# <script>\n",
        "# document.getElementById('restartButton').onclick = function() {\n",
        "#   // Display restart message\n",
        "#   // document.getElementById('statusMessage').innerHTML = '<div style=\"color:#4CAF50;\">Attempting to restart session.</div>';\n",
        "\n",
        "#   // Use Google Colab's runtime API to restart\n",
        "#   try {\n",
        "#     google.colab.kernel.invokeFunction('restart_colab', [], {});\n",
        "#   } catch (error) {\n",
        "#     document.getElementById('statusMessage').innerHTML = '<div style=\"color:#FF5733;\">Error: ' + error.message + '</div>';\n",
        "#   }\n",
        "# };\n",
        "# </script>\n",
        "# ''')\n",
        "\n",
        "# Function that will be called by the button\n",
        "def restart_colab():\n",
        "  display(HTML('<div style=\"color:#4CAF50;\">Running runtime.restart_runtime()</div>'))\n",
        "  # This is the most reliable way to restart Colab\n",
        "  runtime.restart_runtime()\n",
        "\n",
        "# Register the callback function\n",
        "from google.colab import output\n",
        "output.register_callback('restart_colab', restart_colab)\n",
        "\n",
        "# Display the button\n",
        "#display(restart_button)\n",
        "\n",
        "# The rest in this step is a version of the above prepared by Claude AI. We can delete this if it's not helpful.\n",
        "# First, check CUDA version available in Colab\n",
        "#!nvidia-smi\n",
        "\n",
        "# Uninstall conflicting packages more selectively\n",
        "#!pip uninstall -y tensorflow tensorflow-decision-forests thinc spacy flax optax jax jaxlib\n",
        "\n",
        "# Install compatible NumPy and scikit-learn versions first\n",
        "#!pip install numpy==1.24.4 scikit-learn==1.2.2 --force-reinstall\n",
        "\n",
        "# Install RAPIDS with proper error handling and fewer dependencies at once\n",
        "#!pip install -q --extra-index-url=https://pypi.nvidia.com cudf-cu12==25.2.* cuml-cu12==25.2.* rmm-cu12==25.2.*\n",
        "\n",
        "# Check if installation was successful\n",
        "#!python -c \"import cuml; print('cuML version:', cuml.__version__)\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.24.1 # Required for import cudf\n",
        "#!pip install numpy==1.24.4 # Not working with import cudf yet.\n",
        "\n",
        "# The \"Restart Session\" button may not work yet. Click the one in the popup instead.\n",
        "# Click \"Restart Session\" when it pops up. If an error occurs, run the cell again after restarting the session."
      ],
      "metadata": {
        "id": "5kI4_GQOijX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(\"Root directories:\", os.listdir())\n"
      ],
      "metadata": {
        "id": "VV3iQKIW-UYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning up old report folder before each run\n",
        "import shutil, os\n",
        "\n",
        "to_clear = [\"report\"]\n",
        "\n",
        "for d in to_clear:\n",
        "    if os.path.exists(d):\n",
        "        shutil.rmtree(d)\n",
        "        print(f\"Removed old directory: {d}\")\n",
        "\n",
        "# Recreate the clean report folder\n",
        "os.makedirs(\"report\", exist_ok=True)\n",
        "print(\"Recreated clean 'report/' folder\")\n"
      ],
      "metadata": {
        "id": "1hcBr5pD98-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzezuMJhraVG"
      },
      "outputs": [],
      "source": [
        "save_training = False\n",
        "\n",
        "# Required libraries\n",
        "import os # Tarun 07/27/25\n",
        "import cudf\n",
        "import cuml\n",
        "import cupy as cp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import os\n",
        "import regex as re\n",
        "import logging\n",
        "import pickle\n",
        "import csv\n",
        "import requests\n",
        "import yaml\n",
        "import ipywidgets as widgets\n",
        "import pprint\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import base64\n",
        "import json\n",
        "import re\n",
        "from pathlib import Path\n",
        "import time # Tarun 6/2/25\n",
        "\n",
        "from google.colab import _message\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "from io import StringIO\n",
        "from collections import OrderedDict\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "\n",
        "os.makedirs(\"report\", exist_ok=True) # Tarun 07/27/25\n",
        "\n",
        "print(\" All imports successful. GPU ready for cuML and cuDF!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdJKwgi77Lsi"
      },
      "outputs": [],
      "source": [
        "# GPU-Optimized Model Imports\n",
        "from cuml.ensemble import RandomForestClassifier\n",
        "from cuml.linear_model import LogisticRegression\n",
        "from cuml.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier   # MLP remains CPU-based\n",
        "from xgboost import XGBClassifier                   # Will set GPU parameters during model creation\n",
        "from imblearn.over_sampling import SMOTE            # SMOTE stays on CPU\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_curve, roc_auc_score\n",
        "from xgboost import plot_importance\n",
        "\n",
        "print(\" Runtime environment is ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "REPORT_FOLDER = \"report\"  # Default path to the report folder in colab left-nav.\n",
        "\n",
        "def setup_report_folder(report_folder=REPORT_FOLDER):\n",
        "    \"\"\"\n",
        "    Create the report folder if it doesn't exist and download the report.html template and save as index.html.\n",
        "    Returns the number of files in the folder.\n",
        "    \"\"\"\n",
        "    # Create the report folder if it doesn't exist\n",
        "    if not os.path.exists(report_folder):\n",
        "        os.makedirs(report_folder)\n",
        "        print(f\"Created new directory: {report_folder}\")\n",
        "\n",
        "    # Check if index.html exists, if not download it\n",
        "    index_file_path = os.path.join(report_folder, \"index.html\")\n",
        "    if not os.path.exists(index_file_path):\n",
        "        template_url = \"https://raw.githubusercontent.com/ModelEarth/localsite/refs/heads/main/start/template/report.html\"\n",
        "        try:\n",
        "            response = requests.get(template_url)\n",
        "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
        "\n",
        "            with open(index_file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(response.text)\n",
        "            print(f\"Downloaded index.html template to {index_file_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading template: {e}\")\n",
        "\n",
        "    add_readme_to_report_folder(report_folder)\n",
        "\n",
        "def add_readme_to_report_folder(report_folder=REPORT_FOLDER):\n",
        "    \"\"\"\n",
        "    Create a README.md file in the report folder if it doesn't exist yet.\n",
        "    \"\"\"\n",
        "    readme_path = os.path.join(report_folder, \"README.md\")\n",
        "\n",
        "    if not os.path.exists(readme_path):\n",
        "        readme_content = \"# Run Models Report\\n\\nThis folder contains generated reports from model executions.\"\n",
        "\n",
        "        with open(readme_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(readme_content)\n",
        "        print(f\"Created README.md in {report_folder}\")\n",
        "\n",
        "    return readme_path\n",
        "\n",
        "setup_report_folder(REPORT_FOLDER)"
      ],
      "metadata": {
        "id": "i9jwqUTmU_10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "fE9Xji693Fs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "markdown_lines = []"
      ],
      "metadata": {
        "id": "wydSc8N4Popq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EstYTopG4IH"
      },
      "source": [
        "\n",
        "# Parameter Loader and Editor UI\n",
        "\n",
        "This section builds an interactive user interface (UI) for loading, editing, and comparing YAML-based parameter files.\n",
        "\n",
        "**Main functionalities:**\n",
        "- Load available parameter sets from a remote CSV file (name → link).\n",
        "- Display the URL and YAML contents of the selected parameter set.\n",
        "- Allow users to edit YAML content directly in a text box.\n",
        "- Detect and display:\n",
        "  - Changes in the selected parameter source URL.\n",
        "  - Differences between the previous and current remote YAML defaults.\n",
        "  - Changes made to the YAML content in the text box.\n",
        "- Safely update and store the current parameter state for further usage.\n",
        "- Handle special cases like converting a single model string into a list.\n",
        "- Expose key values like `param` (object-based access) and `save_training` (boolean flag) for downstream workflows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhdoU3hXI0Wi"
      },
      "outputs": [],
      "source": [
        "# @title 🔧 Parameter Widget Setup { display-mode: \"code\" }\n",
        "models = ['LR','RFC', 'RBF', 'SVM', 'MLP', 'XGBoost']\n",
        "\n",
        "with open(os.path.join(REPORT_FOLDER, \"model-options.csv\"), 'w', newline='') as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  writer.writerow(['model_name'])\n",
        "  for model in models:\n",
        "    writer.writerow([model])\n",
        "\n",
        "# ----------- Functions -------------\n",
        "def load_parameter_paths_csv(url):\n",
        "    \"\"\"\n",
        "    Download a CSV file from the given URL, read its contents, and return\n",
        "    a dictionary where each entry maps the first column (name)\n",
        "    to the second column (link).\n",
        "    \"\"\"\n",
        "    resp = requests.get(url)\n",
        "    resp.raise_for_status()\n",
        "    reader = csv.reader(StringIO(resp.text))\n",
        "    return {name: link for name, link in reader if len((name, link)) == 2}\n",
        "\n",
        "def compute_diffs(dict_a, dict_b):\n",
        "    \"\"\"\n",
        "    Compare two dictionaries and return a list of differences.\n",
        "    Each difference is a tuple: (key, old_value, new_value).\n",
        "    \"\"\"\n",
        "    diffs = []\n",
        "    for key in sorted(set(dict_a) | set(dict_b)):\n",
        "        old = dict_a.get(key)\n",
        "        new = dict_b.get(key)\n",
        "        if old != new:\n",
        "            diffs.append((key, old, new))\n",
        "    return diffs\n",
        "\n",
        "def pretty_print_diff(title, diffs):\n",
        "    \"\"\"\n",
        "    Nicely format and print differences with separate old/new fields.\n",
        "    \"\"\"\n",
        "    if not diffs:\n",
        "        return\n",
        "    print(f\"\\n=== {title} ===\")\n",
        "    for key, old, new in diffs:\n",
        "        print(f\"• {key}:\")\n",
        "        print(f\"    Old: {pprint.pformat(old, indent=8)}\")\n",
        "        print(f\"    New: {pprint.pformat(new, indent=8)}\\n\")\n",
        "\n",
        "class DictToObject:\n",
        "    \"\"\"\n",
        "    Helper class that recursively converts a dictionary into an object\n",
        "    with attributes, allowing access with dot notation.\n",
        "    \"\"\"\n",
        "    def __init__(self, d):\n",
        "        for k, v in d.items():\n",
        "            if isinstance(v, dict):\n",
        "                v = DictToObject(v)\n",
        "            setattr(self, k, v)\n",
        "    def __getattr__(self, name):\n",
        "        return None\n",
        "\n",
        "# Melody 06/26/2025\n",
        "def save_parameters_to_report():\n",
        "  \"\"\"\n",
        "  Save current parameters to report/parameters.yaml\n",
        "  Reuses existing report folder setup logic\n",
        "  \"\"\"\n",
        "  setup_report_folder(REPORT_FOLDER)\n",
        "  current_params = last_edited_dict.copy()\n",
        "  selected_models = [cb.description for cb in model_checkboxes if cb.value]\n",
        "\n",
        "  if selected_models:\n",
        "    current_params['models'] = selected_models\n",
        "\n",
        "  yaml_file = os.path.join(REPORT_FOLDER, 'parameters.yaml')\n",
        "\n",
        "  with open(yaml_file, 'w', encoding='utf-8') as f:\n",
        "    yaml.safe_dump(current_params, f, sort_keys=False)\n",
        "  print(f'Parameters saved to {yaml_file}')\n",
        "\n",
        "# --- Load Parameter Paths & Default Values ---\n",
        "parameter_csv_url = (\n",
        "    'https://raw.githubusercontent.com/ModelEarth/RealityStream/main/parameters/parameter-paths.csv'\n",
        ")\n",
        "parameter_paths = load_parameter_paths_csv(parameter_csv_url)\n",
        "\n",
        "# Pick the first entry as the default\n",
        "default_name = next(iter(parameter_paths))\n",
        "default_url  = parameter_paths[default_name]\n",
        "\n",
        "# Load Model Names from CSV\n",
        "model_names = []\n",
        "with open(os.path.join(REPORT_FOLDER, \"model-options.csv\"), 'r') as f:\n",
        "    reader = csv.DictReader(f)\n",
        "    for row in reader:\n",
        "        model_names.append(row['model_name'])\n",
        "\n",
        "# --- Load and process the default YAML content ---\n",
        "default_yaml_text = requests.get(default_url).text\n",
        "default_yaml_dict = yaml.safe_load(default_yaml_text) or {}\n",
        "\n",
        "# Extract and process default models\n",
        "default_models = default_yaml_dict.get('models', [])\n",
        "if isinstance(default_models, str):\n",
        "    default_models = [default_models]\n",
        "default_models_lower = [model.lower() for model in default_models]\n",
        "\n",
        "# Remove 'models' key from the YAML dictionary\n",
        "default_yaml_dict.pop('models', None)\n",
        "\n",
        "# Convert the modified dictionary back to a YAML string\n",
        "processed_yaml_text = yaml.safe_dump(default_yaml_dict, sort_keys=False)\n",
        "\n",
        "# --- Widget Definitions ---\n",
        "\n",
        "# Dropdown to select which parameter set to load\n",
        "chooseParams_widget = widgets.Dropdown(\n",
        "    options=list(parameter_paths.keys()),\n",
        "    value=default_name,\n",
        "    description='Params Path'\n",
        ")\n",
        "\n",
        "# Text field showing the URL of the selected YAML file\n",
        "parametersSource_widget = widgets.Text(\n",
        "    value=default_url,\n",
        "    description='Params From',\n",
        "    layout=widgets.Layout(width='1200px')\n",
        ")\n",
        "\n",
        "# Text area allowing inline editing of the YAML content\n",
        "params_widget = widgets.Textarea(\n",
        "    value=processed_yaml_text,\n",
        "    description='Params',\n",
        "    layout=widgets.Layout(width='1200px', height='200px')\n",
        ")\n",
        "\n",
        "# Button to trigger loading and diffing\n",
        "apply_button = widgets.Button(\n",
        "    description='Update',\n",
        "    button_style='primary'\n",
        ")\n",
        "\n",
        "# Output area to display diffs and status\n",
        "output = widgets.Output()\n",
        "\n",
        "# --- Global State: Last URL and Parameter Content ---\n",
        "\n",
        "# Track the last-used URL and parsed dictionaries,\n",
        "# so we can diff against them on each Update click\n",
        "last_url         = default_url\n",
        "last_remote_dict = yaml.safe_load(requests.get(default_url).text) or {}\n",
        "last_params_text = processed_yaml_text\n",
        "last_edited_dict = default_yaml_dict\n",
        "default_models = last_remote_dict.get('models', [])\n",
        "if isinstance(default_models, str):\n",
        "    default_models = [default_models]\n",
        "default_models_lower = [model.lower() for model in default_models]\n",
        "\n",
        "# Flag to track if the user has edited the params_widget\n",
        "user_edited = False\n",
        "\n",
        "# --- Create Model Checkboxes ---\n",
        "\n",
        "model_checkboxes = []\n",
        "for name in model_names:\n",
        "    checked = name.lower() in default_models_lower\n",
        "    cb = widgets.Checkbox(value=checked, description=name)\n",
        "    model_checkboxes.append(cb)\n",
        "\n",
        "model_selection_box = widgets.VBox(model_checkboxes)\n",
        "\n",
        "# --- Event Callbacks ---\n",
        "\n",
        "def on_path_change(change):\n",
        "    \"\"\"\n",
        "    When the dropdown selection changes, update the URL field\n",
        "    and load the new YAML into the editable text area.\n",
        "    \"\"\"\n",
        "    if change['name'] == 'value' and change['type'] == 'change':\n",
        "        name = change['new']\n",
        "        url  = parameter_paths[name]\n",
        "        parametersSource_widget.value = url\n",
        "        yaml_text = requests.get(url).text\n",
        "        yaml_dict = yaml.safe_load(yaml_text) or {}\n",
        "\n",
        "        # Update default models\n",
        "        global default_models\n",
        "        default_models = yaml_dict.get('models', [])\n",
        "        if isinstance(default_models, str):\n",
        "            default_models = [default_models]\n",
        "        default_models_lower = [model.lower() for model in default_models]\n",
        "\n",
        "        # Update checkboxes\n",
        "        for cb in model_checkboxes:\n",
        "            cb.value = cb.description.lower() in default_models_lower\n",
        "\n",
        "        # Remove 'models' key from the YAML dictionary\n",
        "        yaml_dict.pop('models', None)\n",
        "\n",
        "        # Update the text area with the modified YAML\n",
        "        params_widget.value = yaml.safe_dump(yaml_dict, sort_keys=False)\n",
        "\n",
        "        # Reset the user_edited flag\n",
        "        global user_edited\n",
        "        user_edited = False\n",
        "\n",
        "        save_parameters_to_report()\n",
        "\n",
        "chooseParams_widget.observe(on_path_change)\n",
        "\n",
        "def on_params_change(change):\n",
        "    \"\"\"\n",
        "    Set the user_edited flag to True when the user edits the params_widget.\n",
        "    \"\"\"\n",
        "    global user_edited\n",
        "    user_edited = True\n",
        "\n",
        "params_widget.observe(on_params_change, names='value')\n",
        "\n",
        "def on_update_clicked(_):\n",
        "    \"\"\"\n",
        "    Each time the Update button is clicked:\n",
        "    1. Compare the edited YAML text to the last edit and print any key/value changes.\n",
        "    2. Compare the current URL to the last URL and print any change.\n",
        "    3. Diff the remote defaults for both old & new URLs.\n",
        "    4. Update the 'last_' state variables for the next click.\n",
        "    \"\"\"\n",
        "    global last_url, last_remote_dict, last_params_text, last_edited_dict, param, save_training, default_models, user_edited\n",
        "\n",
        "    with output:\n",
        "        clear_output()\n",
        "\n",
        "        # Capture current values first\n",
        "        current_url = parametersSource_widget.value\n",
        "        current_text = params_widget.value\n",
        "        print(\"\\n\")  # Space before text\n",
        "\n",
        "        # 1) YAML content edit detection FIRST\n",
        "        if user_edited:\n",
        "            try:\n",
        "                current_edit = yaml.safe_load(current_text) or {}\n",
        "            except yaml.YAMLError as e:\n",
        "                print(f\"Error parsing edited YAML: {e}\")\n",
        "                return\n",
        "            content_diffs = compute_diffs(last_edited_dict, current_edit)\n",
        "            if content_diffs:\n",
        "                pretty_print_diff(\"YAML edits since last update\", content_diffs)\n",
        "            else:\n",
        "                print(\"YAML text changed but no key/value differences.\\n\")\n",
        "            last_params_text = current_text\n",
        "            last_edited_dict = current_edit\n",
        "            user_edited = False\n",
        "        else:\n",
        "            print(\"YAML content unchanged since last update.\\n\")\n",
        "\n",
        "        # 2) URL change detection\n",
        "        if current_url != last_url:\n",
        "            print(f\"\\n=== URL changed ===\\n\")\n",
        "            print(f\"  {last_url!r} → {current_url!r}\\n\")\n",
        "            try:\n",
        "                new_remote = yaml.safe_load(requests.get(current_url).text) or {}\n",
        "            except Exception as e:\n",
        "                print(f\"Error fetching new remote parameters: {e}\")\n",
        "                return\n",
        "            path_diffs = compute_diffs(last_remote_dict, new_remote)\n",
        "            if path_diffs:\n",
        "                pretty_print_diff(\"Default parameters changed between URLs\", path_diffs)\n",
        "            else:\n",
        "                print(\"No default-parameter differences between those URLs.\\n\")\n",
        "            last_url = current_url\n",
        "            last_remote_dict = new_remote\n",
        "        else:\n",
        "            print(f\"URL unchanged: {current_url!r}\\n\")\n",
        "\n",
        "        # 3) Update models from checkboxes\n",
        "        selected_models = [cb.description for cb in model_checkboxes if cb.value]\n",
        "        if selected_models:\n",
        "            last_edited_dict['models'] = selected_models\n",
        "            print(f\"Selected models: {selected_models}\")\n",
        "        else:\n",
        "            print(\"No models selected.\")\n",
        "\n",
        "        # Compare selected models with default models (case-insensitive)\n",
        "        selected_models_lower = [model.lower() for model in selected_models]\n",
        "        default_models_lower = [model.lower() for model in default_models]\n",
        "\n",
        "        added_models = [model for model in selected_models if model.lower() not in default_models_lower]\n",
        "        removed_models = [model for model in default_models if model.lower() not in selected_models_lower]\n",
        "\n",
        "        if added_models or removed_models:\n",
        "            print(\"\\n=== Model Selection Changes ===\")\n",
        "            if added_models:\n",
        "                print(f\"Added models: {added_models}\")\n",
        "            if removed_models:\n",
        "                print(f\"Removed models: {removed_models}\")\n",
        "        else:\n",
        "            print(\"Model selection unchanged.\")\n",
        "\n",
        "        # Update default_models for next comparison\n",
        "        default_models = selected_models.copy()\n",
        "\n",
        "        # 4) Build updated param and save_training\n",
        "        param = DictToObject(OrderedDict(last_edited_dict))\n",
        "        save_training = getattr(param, 'save_training', False)\n",
        "\n",
        "        save_pickle = getattr(param, 'save_pickle', False)  # Tarun\n",
        "        print(f\"save_pickle set to: {save_pickle}\")  # Tarun\n",
        "\n",
        "        # Changes tarun\n",
        "        # Define mapping of model keys to full import\n",
        "\n",
        "        import importlib\n",
        "\n",
        "        model_import_paths = {\n",
        "            \"RFC\": \"sklearn.ensemble.RandomForestClassifier\",\n",
        "            \"RBF\": \"sklearn.ensemble.RandomForestClassifier\",  # alias\n",
        "            \"LR\": \"sklearn.linear_model.LogisticRegression\",\n",
        "            \"LogisticRegression\": \"sklearn.linear_model.LogisticRegression\",\n",
        "            \"SVM\": \"sklearn.svm.SVC\",\n",
        "            \"MLP\": \"sklearn.neural_network.MLPClassifier\",\n",
        "            \"XGBoost\": \"xgboost.XGBClassifier\"\n",
        "        }\n",
        "\n",
        "\n",
        "        # Create a dictionary to store dynamically imported model classes\n",
        "        loaded_model_classes = {}\n",
        "\n",
        "        # Use param_dict for safe access\n",
        "        requested_models = last_edited_dict.get(\"models\", [])\n",
        "\n",
        "        for model_name in requested_models:\n",
        "            if model_name not in model_import_paths:\n",
        "                print(f\" Unknown model: {model_name}\")\n",
        "                continue\n",
        "\n",
        "            full_path = model_import_paths[model_name]\n",
        "            module_name, class_name = full_path.rsplit('.', 1)\n",
        "\n",
        "            try:\n",
        "                module = importlib.import_module(module_name)\n",
        "                model_class = getattr(module, class_name)\n",
        "                loaded_model_classes[model_name] = model_class\n",
        "                print(f\" Loaded {model_name} from {module_name}\")\n",
        "            except (ImportError, AttributeError) as e:\n",
        "                print(f\" Failed to import {model_name}: {e}\")\n",
        "\n",
        "        # 5) Fix single model case: always make models a list\n",
        "        if isinstance(last_edited_dict.get(\"models\"), str):\n",
        "            last_edited_dict[\"models\"] = [last_edited_dict[\"models\"]]\n",
        "            param = DictToObject(OrderedDict(last_edited_dict))  # Rebuild after fix\n",
        "\n",
        "        save_parameters_to_report()\n",
        "\n",
        "apply_button.on_click(on_update_clicked)\n",
        "\n",
        "# --- Display the UI ---\n",
        "\n",
        "ui = widgets.VBox([\n",
        "    chooseParams_widget,\n",
        "    parametersSource_widget,\n",
        "    params_widget,\n",
        "    model_selection_box,\n",
        "    apply_button,\n",
        "    output\n",
        "])\n",
        "display(ui)\n",
        "on_update_clicked(None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alauCxr5yHF7"
      },
      "outputs": [],
      "source": [
        "# Assuming 'param' is an instance of DictToObject from previous code blocks\n",
        "# Define necessary adjustments to your setup\n",
        "\n",
        "# Settings\n",
        "model_name = \"RandomForest\"  # Specify the model to be trained\n",
        "all_model_list = [\"LogisticRegression\", \"SVM\", \"MLP\", \"RandomForest\", \"XGBoost\"]  # All usable models\n",
        "assert model_name in all_model_list, \"Model not supported\"\n",
        "valid_report_list = [\"RandomForest\", \"XGBoost\"]  # Valid models for feature-importance report\n",
        "\n",
        "random_state = 42  # Random state for reproducibility\n",
        "# print(param.features.path)\n",
        "#print(param.targets.__dict__)\n",
        "\n",
        "# Tarun changes\n",
        "# Dynamically import only the models specified in param.models\n",
        "available_model_classes = {}\n",
        "\n",
        "# Normalize all names to lowercase to match YAML inputs\n",
        "requested_models = [m.lower() for m in last_edited_dict.get('models', [])]\n",
        "\n",
        "if 'randomforest' in requested_models:\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    available_model_classes['RandomForest'] = RandomForestClassifier\n",
        "\n",
        "if 'svm' in requested_models:\n",
        "    from sklearn.svm import SVC\n",
        "    available_model_classes['SVM'] = SVC\n",
        "\n",
        "if 'logisticregression' in requested_models:\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    available_model_classes['LogisticRegression'] = LogisticRegression\n",
        "\n",
        "if 'mlp' in requested_models:\n",
        "    from sklearn.neural_network import MLPClassifier\n",
        "    available_model_classes['MLP'] = MLPClassifier\n",
        "\n",
        "if 'xgboost' in requested_models:\n",
        "    import xgboost as xgb\n",
        "    available_model_classes['XGBoost'] = xgb.XGBClassifier\n",
        "\n",
        "print(param.targets.path)\n",
        "# Access the 'path' key within the 'targets' object safely\n",
        "target_url = param.targets.path\n",
        "target_df = pd.read_csv(target_url)\n",
        "print(target_df.head())\n",
        "\n",
        "# Tarun Changes\n",
        "# Normalize and search for “fips” in any case or with stray whitespace\n",
        "cols = [col.strip() for col in target_df.columns]\n",
        "match = next((col for col in cols if col.lower() == \"fips\"), None)\n",
        "if not match:\n",
        "    raise ValueError(\"No valid location column found (expected something like 'FIPS').\")\n",
        "location_column = match\n",
        "print(f\"Location column identified: {location_column!r}\")\n",
        "\n",
        "\n",
        "# Dynamically identify the location column\n",
        "# location_columns = [\"Country\", \"State\", \"Fips\", \"Zip\", \"Voxel\"]\n",
        "# location_column = next((col for col in target_df.columns if col in location_columns), None)\n",
        "# if not location_column:\n",
        "#     raise ValueError(\"No valid location column found in the target dataset.\")\n",
        "# print(f\"Location column identified: {location_column}\")\n",
        "\n",
        "# Dynamically identify the target column\n",
        "# TO DO: Convert all incoming to lowercase to column name \"target\" also works.\n",
        "target_column = \"Target\" if \"Target\" in target_df.columns else None\n",
        "if not target_column:\n",
        "    #raise ValueError(\"The 'Target' column is not found in the target dataset.\")\n",
        "    print(\"The 'Target' column is not found in the target dataset.\")\n",
        "print(f\"Target column identified: {target_column}\")\n",
        "\n",
        "# Directory Information\n",
        "dataset_name = \"Name needs to be added\"\n",
        "merged_save_dir = f\"../process/{dataset_name}/states-{target_column}-{dataset_name}\"  # Directory for state-separate dataset\n",
        "full_save_dir = f\"../output/{dataset_name}/training\"  # Directory for the integrated dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_df"
      ],
      "metadata": {
        "id": "Pwyg79YJs26J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIth7Fegr26z"
      },
      "outputs": [],
      "source": [
        "# STEP: Create Functions\n",
        "def rename_columns(df, year):\n",
        "    rename_mapping = {}\n",
        "    for column in df.columns:\n",
        "      if column not in df.columns[:2]:\n",
        "          new_column_name = column + f'-{year}'\n",
        "          rename_mapping[column] = new_column_name\n",
        "    df.rename(columns=rename_mapping, inplace=True)\n",
        "\n",
        "def check_directory(directory_path): # Check whether the given directory exists, if not, then create it\n",
        "    if not os.path.exists(directory_path):\n",
        "        try:\n",
        "            os.makedirs(directory_path)\n",
        "            print(f\"Directory '{directory_path}' created successfully by check_directory.\")\n",
        "        except OSError as e:\n",
        "            print(f\"Error creating directory '{directory_path}': {e}\")\n",
        "    else:\n",
        "        print(\"Current working directory:\", os.getcwd())\n",
        "        print(\"View under the folder icon which is followed by 2 dots..\")\n",
        "        print(f\"check_directory '{directory_path}' already exists.\")\n",
        "    return directory_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqnhVJEwWSRR"
      },
      "source": [
        "# Model functions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"datacommons-client[Pandas]\" --upgrade --quiet"
      ],
      "metadata": {
        "id": "oyQ9rze7j7Lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datacommons_client import DataCommonsClient\n",
        "import time\n",
        "\n",
        "client = DataCommonsClient(api_key=\"AIzaSyCTI4Xz-UW_G2Q2RfknhcfdAnTHq5X5XuI\")"
      ],
      "metadata": {
        "id": "DE-wNlf8j-an"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_forest_cover_dataframe(dcid: str, level: str = \"County\") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Pulls LandCoverFraction_Forest data for all child counties under the given dcid.\n",
        "\n",
        "    Parameters:\n",
        "    - dcid: The DCID of the parent region (e.g., \"geoId/13\" for Georgia)\n",
        "    - level: Child place type to pull data for (default: \"County\")\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame with forest cover data\n",
        "    \"\"\"\n",
        "    from datacommons_client import DataCommonsClient\n",
        "    import pandas as pd\n",
        "\n",
        "    client = DataCommonsClient(api_key=\"AIzaSyCTI4Xz-UW_G2Q2RfknhcfdAnTHq5X5XuI\")\n",
        "\n",
        "    print(f\"Getting child {level}s for {dcid}...\")\n",
        "    try:\n",
        "        child_places = client.node.fetch_place_children(dcid, children_type=level)[dcid]\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching children for {dcid}: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    if not child_places:\n",
        "        print(\"No child places found.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Extract DCIDs only\n",
        "    child_dcids = [place[\"dcid\"] for place in child_places]\n",
        "    print(f\"Found {len(child_dcids)} {level}s\")\n",
        "\n",
        "    print(\"Fetching forest cover data...\")\n",
        "    df = client.observations_dataframe(\n",
        "        variable_dcids=[\"LandCoverFraction_Forest\"],\n",
        "        date=\"all\",\n",
        "        entity_dcids=child_dcids\n",
        "    )\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"No forest cover data found.\")\n",
        "    else:\n",
        "        print(f\"Retrieved {len(df)} rows of forest data.\")\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "3_45HHyloO7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Get full forest data for Georgia\n",
        "df = get_forest_cover_dataframe(\"geoId/13\")\n",
        "\n",
        "\"\"\"\n",
        "Below is the complete list of U.S. state geoId codes you can use as input:\n",
        "\n",
        "    \"Alabama\": \"geoId/01\",\n",
        "    \"Alaska\": \"geoId/02\",\n",
        "    \"Arizona\": \"geoId/04\",\n",
        "    \"Arkansas\": \"geoId/05\",\n",
        "    \"California\": \"geoId/06\",\n",
        "    \"Colorado\": \"geoId/08\",\n",
        "    \"Connecticut\": \"geoId/09\",\n",
        "    \"Delaware\": \"geoId/10\",\n",
        "    \"Florida\": \"geoId/12\",\n",
        "    \"Georgia\": \"geoId/13\",\n",
        "    \"Hawaii\": \"geoId/15\",\n",
        "    \"Idaho\": \"geoId/16\",\n",
        "    \"Illinois\": \"geoId/17\",\n",
        "    \"Indiana\": \"geoId/18\",\n",
        "    \"Iowa\": \"geoId/19\",\n",
        "    \"Kansas\": \"geoId/20\",\n",
        "    \"Kentucky\": \"geoId/21\",\n",
        "    \"Louisiana\": \"geoId/22\",\n",
        "    \"Maine\": \"geoId/23\",\n",
        "    \"Maryland\": \"geoId/24\",\n",
        "    \"Massachusetts\": \"geoId/25\",\n",
        "    \"Michigan\": \"geoId/26\",\n",
        "    \"Minnesota\": \"geoId/27\",\n",
        "    \"Mississippi\": \"geoId/28\",\n",
        "    \"Missouri\": \"geoId/29\",\n",
        "    \"Montana\": \"geoId/30\",\n",
        "    \"Nebraska\": \"geoId/31\",\n",
        "    \"Nevada\": \"geoId/32\",\n",
        "    \"New Hampshire\": \"geoId/33\",\n",
        "    \"New Jersey\": \"geoId/34\",\n",
        "    \"New Mexico\": \"geoId/35\",\n",
        "    \"New York\": \"geoId/36\",\n",
        "    \"North Carolina\": \"geoId/37\",\n",
        "    \"North Dakota\": \"geoId/38\",\n",
        "    \"Ohio\": \"geoId/39\",\n",
        "    \"Oklahoma\": \"geoId/40\",\n",
        "    \"Oregon\": \"geoId/41\",\n",
        "    \"Pennsylvania\": \"geoId/42\",\n",
        "    \"Rhode Island\": \"geoId/44\",\n",
        "    \"South Carolina\": \"geoId/45\",\n",
        "    \"South Dakota\": \"geoId/46\",\n",
        "    \"Tennessee\": \"geoId/47\",\n",
        "    \"Texas\": \"geoId/48\",\n",
        "    \"Utah\": \"geoId/49\",\n",
        "    \"Vermont\": \"geoId/50\",\n",
        "    \"Virginia\": \"geoId/51\",\n",
        "    \"Washington\": \"geoId/53\",\n",
        "    \"West Virginia\": \"geoId/54\",\n",
        "    \"Wisconsin\": \"geoId/55\",\n",
        "    \"Wyoming\": \"geoId/56\"\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Step 2: Clean and rename\n",
        "df[\"entity\"] = df[\"entity\"].str[6:]\n",
        "df_forest = df[[\"entity\", \"entity_name\", \"date\", \"value\"]].copy()\n",
        "df_forest = df_forest.rename(columns={\n",
        "    \"entity\": \"Fips\",\n",
        "    \"entity_name\": \"county_name\",\n",
        "    \"date\": \"year\",\n",
        "    \"value\": \"Forest_Cover_Percent\"\n",
        "})\n",
        "\n",
        "\n",
        "# Step 3: Filter for a specific year\n",
        "\"\"\"\n",
        "    Forest Cover Data Years:\n",
        "\n",
        "    Available years: 2015, 2016, 2017, 2018, 2019\n",
        "\n",
        "    Forest cover data is not available before 2015 or after 2019.\n",
        "    If filtering by year, please choose a value from this range.\n",
        "\n",
        "    Example:\n",
        "    year_to_filter = \"2019\"\n",
        "\"\"\"\n",
        "year_to_filter = \"2019\"  # <-- user can input this\n",
        "df_filtered = df_forest[df_forest[\"year\"] == year_to_filter]\n",
        "\n",
        "\n",
        "\n",
        "# Preview the filtered DataFrame\n",
        "df_filtered.head()\n",
        "\n",
        "\n",
        "# Step 4: Preview\n",
        "\n",
        "print(df_filtered.shape)"
      ],
      "metadata": {
        "id": "ThytFRavkQo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3de733d4"
      },
      "source": [
        "## Task - Savar\n",
        "Identify the top ten counties in each state likely to have declining tree canopy."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "years_to_study = range(2015, 2020)\n",
        "yearly_forest_data = {}\n",
        "\n",
        "for year in years_to_study:\n",
        "    df_year = df_forest[df_forest[\"year\"].astype(int) == year].copy()\n",
        "    yearly_forest_data[year] = df_year\n",
        "    print(f\"Created dataframe for year {year} with shape: {df_year.shape}\")\n",
        "\n",
        "# You can access the dataframes using the dictionary, e.g., yearly_forest_data[2015]"
      ],
      "metadata": {
        "id": "u4E_9f3pttPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logically calculated decline in forest cover averaged over 4 years"
      ],
      "metadata": {
        "id": "tvwB3kcHuUiV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d6655d6"
      },
      "source": [
        "# Get data for 2015 and 2019\n",
        "df_2015 = yearly_forest_data[2015]\n",
        "df_2019 = yearly_forest_data[2019]\n",
        "\n",
        "# Merge the dataframes on Fips\n",
        "df_merged = pd.merge(df_2015[['Fips', 'county_name', 'Forest_Cover_Percent']],\n",
        "                     df_2019[['Fips', 'Forest_Cover_Percent']],\n",
        "                     on='Fips',\n",
        "                     suffixes=('_2015', '_2019'))\n",
        "\n",
        "# Calculate the forest cover change and take the absolute value\n",
        "df_merged['Forest_Cover_Drop'] = abs(df_merged['Forest_Cover_Percent_2019'] - df_merged['Forest_Cover_Percent_2015'])\n",
        "\n",
        "# Sort by change to find the largest declines\n",
        "df_declining = df_merged.sort_values(by='Forest_Cover_Drop', ascending=False).head(10)\n",
        "\n",
        "print(\"Top 10 counties with the largest forest cover decline (2015-2019):\")\n",
        "df_declining"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing process to calculate top 10 counties methematically"
      ],
      "metadata": {
        "id": "CmonM6-m4liZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d40e5c2"
      },
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "def process_state_forest_decline(state_geoId: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Identifies the top 10 counties with the largest forest cover decline\n",
        "    for a given state (geoId) between 2015 and 2019.\n",
        "\n",
        "    Parameters:\n",
        "    - state_geoId: The geoId of the state (e.g., \"geoId/13\" for Georgia)\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame with the top 10 declining counties for the state,\n",
        "      including state geoId. Returns an empty DataFrame\n",
        "      if data fetching fails or no counties are found.\n",
        "    \"\"\"\n",
        "    print(f\"\\nProcessing forest decline for state: {state_geoId}\")\n",
        "\n",
        "    # Get forest cover data for the state's counties\n",
        "    df_state_forest = get_forest_cover_dataframe(state_geoId, level=\"County\")\n",
        "\n",
        "    if df_state_forest.empty:\n",
        "        print(f\"No forest data found for {state_geoId}. Skipping.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Filter for years 2015 and 2019 using the 'date' column\n",
        "    df_2015 = df_state_forest[df_state_forest[\"date\"].astype(int) == 2015].copy()\n",
        "    df_2019 = df_state_forest[df_state_forest[\"date\"].astype(int) == 2019].copy()\n",
        "\n",
        "    if df_2015.empty or df_2019.empty:\n",
        "        print(f\"Data for 2015 or 2019 is missing for {state_geoId}. Skipping.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Merge the dataframes on Fips (entity column after renaming)\n",
        "    df_merged = pd.merge(df_2015[['entity', 'entity_name', 'value']],\n",
        "                         df_2019[['entity', 'value']],\n",
        "                         on='entity',\n",
        "                         suffixes=('_2015', '_2019'))\n",
        "\n",
        "    # Rename columns for clarity after merging\n",
        "    df_merged = df_merged.rename(columns={\n",
        "        'entity': 'Fips',\n",
        "        'entity_name': 'county_name',\n",
        "        'value_2015': 'Forest_Cover_Percent_2015',\n",
        "        'value_2019': 'Forest_Cover_Percent_2019'\n",
        "    })\n",
        "\n",
        "\n",
        "    # Calculate the absolute forest cover drop\n",
        "    df_merged['Forest_Cover_Drop'] = abs(df_merged['Forest_Cover_Percent_2019'] - df_merged['Forest_Cover_Percent_2015'])\n",
        "\n",
        "    # Sort by drop to find the largest declines and get top 10\n",
        "    df_declining = df_merged.sort_values(by='Forest_Cover_Drop', ascending=False).head(10).copy()\n",
        "\n",
        "    # Add state geoId\n",
        "    df_declining['state_geoId'] = state_geoId\n",
        "\n",
        "\n",
        "    print(f\"Found top 10 declining counties for {state_geoId}.\")\n",
        "    return df_declining[['state_geoId', 'Fips', 'county_name', 'Forest_Cover_Percent_2015', 'Forest_Cover_Percent_2019', 'Forest_Cover_Drop']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing statewise average drops"
      ],
      "metadata": {
        "id": "DyZdcjMg4sRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "def calculate_average_state_drop(state_geoId: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calculates the average forest cover drop for a state between 2015 and 2019.\n",
        "\n",
        "    Parameters:\n",
        "    - state_geoId: The geoId of the state (e.g., \"geoId/13\" for Georgia)\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame with columns ['geoId', 'average_drop']\n",
        "      containing the calculated values for the input state. Returns an empty\n",
        "      DataFrame if data fetching fails or no counties are found.\n",
        "    \"\"\"\n",
        "    print(f\"\\nCalculating average forest drop for state: {state_geoId}\")\n",
        "\n",
        "    # Get forest cover data for the state's counties\n",
        "    df_state_forest = get_forest_cover_dataframe(state_geoId, level=\"County\")\n",
        "\n",
        "    if df_state_forest.empty:\n",
        "        print(f\"No forest data found for {state_geoId}. Skipping.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Filter for years 2015 and 2019 using the 'date' column\n",
        "    df_2015 = df_state_forest[df_state_forest[\"date\"].astype(int) == 2015].copy()\n",
        "    df_2019 = df_state_forest[df_state_forest[\"date\"].astype(int) == 2019].copy()\n",
        "\n",
        "    if df_2015.empty or df_2019.empty:\n",
        "        print(f\"Data for 2015 or 2019 is missing for {state_geoId}. Skipping.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Rename columns before merging for consistency\n",
        "    df_2015 = df_2015.rename(columns={\n",
        "        'entity': 'Fips',\n",
        "        'value': 'Forest_Cover_Percent'\n",
        "    })\n",
        "    df_2019 = df_2019.rename(columns={\n",
        "        'entity': 'Fips',\n",
        "        'value': 'Forest_Cover_Percent'\n",
        "    })\n",
        "\n",
        "    # Merge the dataframes on Fips\n",
        "    df_merged = pd.merge(df_2015[['Fips', 'Forest_Cover_Percent']],\n",
        "                         df_2019[['Fips', 'Forest_Cover_Percent']],\n",
        "                         on='Fips',\n",
        "                         suffixes=('_2015', '_2019'))\n",
        "\n",
        "\n",
        "    # Calculate the absolute forest cover drop\n",
        "    df_merged['Forest_Cover_Drop'] = abs(df_merged['Forest_Cover_Percent_2019'] - df_merged['Forest_Cover_Percent_2015'])\n",
        "\n",
        "    # Calculate the average drop for the state\n",
        "    average_drop = df_merged['Forest_Cover_Drop'].mean()\n",
        "\n",
        "    print(f\"Average drop for {state_geoId}: {average_drop:.2f}%\")\n",
        "\n",
        "    # Return as a DataFrame\n",
        "    return pd.DataFrame({\n",
        "        'state_geoId': [state_geoId],\n",
        "        'average_drop': [average_drop]\n",
        "    })"
      ],
      "metadata": {
        "id": "xlN2V2N72w2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process test with 4 states"
      ],
      "metadata": {
        "id": "ndQ0XSnV4yHo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07e7f0e0"
      },
      "source": [
        "# List of state geoIds to process (replace with your desired list of states)\n",
        "# Example includes a few states:\n",
        "state_geoIds = [\n",
        "    \"geoId/13\", # Georgia (already used in testing)\n",
        "    \"geoId/36\", # New York\n",
        "    \"geoId/06\", # California\n",
        "    \"geoId/48\"  # Texas\n",
        "]\n",
        "\n",
        "all_states_top_10_declining = pd.DataFrame()\n",
        "average_state_drops = pd.DataFrame()\n",
        "\n",
        "for state_geoId in state_geoIds:\n",
        "    # Process state forest decline and append to the combined DataFrame\n",
        "    top_10_declining_state = process_state_forest_decline(state_geoId)\n",
        "    if not top_10_declining_state.empty:\n",
        "        all_states_top_10_declining = pd.concat([all_states_top_10_declining, top_10_declining_state], ignore_index=True)\n",
        "\n",
        "    # Calculate average state drop and append to the combined DataFrame\n",
        "    average_drop_state = calculate_average_state_drop(state_geoId)\n",
        "    if not average_drop_state.empty:\n",
        "        average_state_drops = pd.concat([average_state_drops, average_drop_state], ignore_index=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming STATE_DICT is already defined in a previous cell\n",
        "# Example STATE_DICT (should match the one in your notebook):\n",
        "STATE_DICT = {\n",
        "    \"AL\": \"Alabama\", \"AK\": \"Alaska\", \"AZ\": \"Arizona\", \"AR\": \"Arkansas\", \"CA\": \"California\", \"CO\": \"Colorado\",\n",
        "    \"CT\": \"Connecticut\", \"DE\": \"Delaware\", \"FL\": \"Florida\", \"GA\": \"Georgia\", \"HI\": \"Hawaii\", \"ID\": \"Idaho\",\n",
        "    \"IL\": \"Illinois\", \"IN\": \"Indiana\", \"IA\": \"Iowa\", \"KS\": \"Kansas\", \"KY\": \"Kentucky\", \"LA\": \"Louisiana\",\n",
        "    \"ME\": \"Maine\", \"MD\": \"Maryland\", \"MA\": \"Massachusetts\", \"MI\": \"Michigan\", \"MN\": \"Minnesota\", \"MS\": \"Mississippi\",\n",
        "    \"MO\": \"Missouri\", \"MT\": \"Montana\", \"NE\": \"Nebraska\", \"NV\": \"Nevada\", \"NH\": \"New Hampshire\", \"NJ\": \"New Jersey\",\n",
        "    \"NM\": \"New Mexico\", \"NY\": \"New York\", \"NC\": \"North Carolina\", \"ND\": \"North Dakota\", \"OH\": \"Ohio\", \"OK\": \"Oklahoma\",\n",
        "    \"OR\": \"Oregon\", \"PA\": \"Pennsylvania\", \"RI\": \"Rhode Island\", \"SC\": \"South Carolina\", \"SD\": \"South Dakota\",\n",
        "    \"TN\": \"Tennessee\", \"TX\": \"Texas\", \"UT\": \"Utah\", \"VT\": \"Vermont\", \"VA\": \"Virginia\", \"WA\": \"Washington\",\n",
        "    \"WV\": \"West Virginia\", \"WI\": \"Wisconsin\", \"WY\": \"Wyoming\",\n",
        "    \"DC\": \"District of Columbia\",\n",
        "    # US Territories\n",
        "    \"AS\": \"American Samoa\", \"GU\": \"Guam\", \"MP\": \"Northern Mariana Islands\", \"PR\": \"Puerto Rico\", \"VI\": \"U.S. Virgin Islands\"\n",
        "}\n",
        "\n",
        "\n",
        "def map_geoId_to_state_name(df, geoid_column='geoId'):\n",
        "    \"\"\"\n",
        "    Maps geoId values in a specified column of a DataFrame to their\n",
        "    corresponding state names using the STATE_DICT.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame.\n",
        "        geoid_column (str): The name of the column containing geoId values.\n",
        "                             Defaults to 'geoId'.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The DataFrame with geoId values replaced by state names,\n",
        "                      or the original DataFrame if the column does not exist.\n",
        "    \"\"\"\n",
        "    if geoid_column not in df.columns:\n",
        "        print(f\"Warning: Column '{geoid_column}' not found in the DataFrame.\")\n",
        "        return df\n",
        "\n",
        "    # Extract state abbreviation from geoId (e.g., 'geoId/13' -> '13')\n",
        "    # Then map the state code (e.g., '13') to the state abbreviation (e.g., 'GA')\n",
        "    # This requires a mapping from state code to abbreviation or updating STATE_DICT keys.\n",
        "    # Assuming STATE_DICT keys are already abbreviations like 'GA', 'NY', etc.\n",
        "    # If geoId format is 'geoId/XX', we need to map XX to the abbreviation first.\n",
        "    # Let's assume for now geoId column contains abbreviations or full geoId strings like 'geoId/13'\n",
        "\n",
        "    # If geoId column contains strings like 'geoId/13', extract the state code '13'\n",
        "    # and then map it to the abbreviation using a reverse lookup or another dictionary.\n",
        "    # For simplicity, let's assume the column contains abbreviations (e.g., 'GA', 'NY')\n",
        "    # or that STATE_DICT keys are designed to handle the format in the DataFrame.\n",
        "\n",
        "    # A robust approach would involve mapping geoId/XX to abbreviation first.\n",
        "    # Let's create a simple mapping for common geoIds to abbreviations for this function's use case.\n",
        "    # This is a simplified example, a more complete mapping might be needed.\n",
        "    geoid_to_abbr = {\n",
        "        \"geoId/01\": \"AL\", \"geoId/02\": \"AK\", \"geoId/04\": \"AZ\", \"geoId/05\": \"AR\", \"geoId/06\": \"CA\",\n",
        "        \"geoId/08\": \"CO\", \"geoId/09\": \"CT\", \"geoId/10\": \"DE\", \"geoId/12\": \"FL\", \"geoId/13\": \"GA\",\n",
        "        \"geoId/15\": \"HI\", \"geoId/16\": \"ID\", \"geoId/17\": \"IL\", \"geoId/18\": \"IN\", \"geoId/19\": \"IA\",\n",
        "        \"geoId/20\": \"KS\", \"geoId/21\": \"KY\", \"geoId/22\": \"LA\", \"geoId/23\": \"ME\", \"geoId/24\": \"MD\",\n",
        "        \"geoId/25\": \"MA\", \"geoId/26\": \"MI\", \"geoId/27\": \"MN\", \"geoId/28\": \"MS\", \"geoId/29\": \"MO\",\n",
        "        \"geoId/30\": \"MT\", \"geoId/31\": \"NE\", \"geoId/32\": \"NV\", \"geoId/33\": \"NH\", \"geoId/34\": \"NJ\",\n",
        "        \"geoId/35\": \"NM\", \"geoId/36\": \"NY\", \"geoId/37\": \"NC\", \"geoId/38\": \"ND\", \"geoId/39\": \"OH\",\n",
        "        \"geoId/40\": \"OK\", \"geoId/41\": \"OR\", \"geoId/42\": \"PA\", \"geoId/44\": \"RI\", \"geoId/45\": \"SC\",\n",
        "        \"geoId/46\": \"SD\", \"geoId/47\": \"TN\", \"geoId/48\": \"TX\", \"geoId/49\": \"UT\", \"geoId/50\": \"VT\",\n",
        "        \"geoId/51\": \"VA\", \"geoId/53\": \"WA\", \"geoId/54\": \"WV\", \"geoId/55\": \"WI\", \"geoId/56\": \"WY\",\n",
        "        \"geoId/11\": \"DC\" # District of Columbia\n",
        "    }\n",
        "\n",
        "\n",
        "    # Apply the mapping\n",
        "    # First map geoId string to abbreviation\n",
        "    df[geoid_column] = df[geoid_column].map(geoid_to_abbr).fillna(df[geoid_column]) # Fallback to original if no mapping\n",
        "\n",
        "    # Then map abbreviation to full state name\n",
        "    df[geoid_column] = df[geoid_column].map(STATE_DICT).fillna(df[geoid_column]) # Fallback to abbreviation if no mapping\n",
        "\n",
        "    return df\n",
        "\n",
        "# Example usage (assuming average_state_drops DataFrame exists):\n",
        "# average_state_drops_with_names = map_geoId_to_state_name(average_state_drops.copy(), geoid_column='geoId')\n",
        "# print(\"\\nAverage State Forest Drops with State Names:\")\n",
        "# display(average_state_drops_with_names)\n",
        "\n",
        "# Example usage with all_states_top_10_declining (assuming it exists):\n",
        "# all_states_top_10_declining_with_names = map_geoId_to_state_name(all_states_top_10_declining.copy(), geoid_column='state_geoId')\n",
        "# print(\"\\nCombined Top 10 Declining Counties with State Names:\")\n",
        "# display(all_states_top_10_declining_with_names.head())"
      ],
      "metadata": {
        "id": "JYWoaLhBnHFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Processing Complete ---\")\n",
        "print(\"\\nCombined Top 10 Declining Counties (all states):\")\n",
        "all_states_top_10_declining=map_geoId_to_state_name(all_states_top_10_declining.copy(), geoid_column='state_geoId')\n",
        "all_states_top_10_declining.head()\n",
        "print(f\"\\nShape of combined top 10 declining counties: {all_states_top_10_declining.shape}\")\n",
        "\n",
        "print(\"\\nAverage State Forest Drops:\")\n",
        "average_state_drops=map_geoId_to_state_name(average_state_drops.copy(), geoid_column='state_geoId')\n",
        "average_state_drops.head()\n",
        "print(f\"\\nShape of average state forest drops: {average_state_drops.shape}\")"
      ],
      "metadata": {
        "id": "40mnS8FhnpWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## County Wise Visualizations"
      ],
      "metadata": {
        "id": "HWYaowHa4-Tj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49886c42"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Ensure the dataframe is sorted for better visualization\n",
        "average_state_drops = average_state_drops.sort_values(by='average_drop', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='state_geoId', y='average_drop', data=average_state_drops, palette='viridis')\n",
        "plt.title('Average Forest Canopy Drop by State (2015-2019)')\n",
        "plt.xlabel('State geoId')\n",
        "plt.ylabel('Average Forest Canopy Drop (%)')\n",
        "plt.xticks(rotation=45, ha='right') # Rotate labels for better readability\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.scatterplot(\n",
        "    data=all_states_top_10_declining,\n",
        "    x='Forest_Cover_Percent_2015',\n",
        "    y='Forest_Cover_Percent_2019',\n",
        "    size='Forest_Cover_Drop',\n",
        "    hue='state_geoId',\n",
        "    sizes=(100, 1000),  # Adjust bubble size range as needed\n",
        "    alpha=0.7\n",
        ")\n",
        "\n",
        "plt.title('County Deforestation (2015-2019) by State')\n",
        "plt.xlabel('Forest Cover Percent (2015)')\n",
        "plt.ylabel('Forest Cover Percent (2019)')\n",
        "plt.grid(True)\n",
        "plt.legend(title='State geoId', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4rn-08Nk3N_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af1ba112"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(data=all_states_top_10_declining, x='state_geoId', y='Forest_Cover_Drop', palette='viridis', hue='state_geoId', legend=False)\n",
        "plt.title('Distribution of Forest Cover Drop by State (2015-2019)')\n",
        "plt.xlabel('State geoId')\n",
        "plt.ylabel('Forest Cover Drop (%)')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Assuming get_forest_cover_dataframe is defined in a previous cell\n",
        "\n",
        "def get_all_years_forest_data_for_state(state_geoId: str, start_year: int = 2015, end_year: int = 2019) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Fetches and combines forest cover data for all years in a specified range\n",
        "    for a single state.\n",
        "\n",
        "    Parameters:\n",
        "    - state_geoId: The geoId of the state (e.g., \"geoId/13\" for Georgia)\n",
        "    - start_year: The starting year (inclusive)\n",
        "    - end_year: The ending year (inclusive)\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame with forest cover data for all years and counties in the state,\n",
        "      or an empty DataFrame if data fetching fails or no data is found.\n",
        "    \"\"\"\n",
        "    print(f\"\\nFetching and combining forest data for state: {state_geoId} from {start_year} to {end_year}\")\n",
        "\n",
        "    # Get forest cover data for the state's counties\n",
        "    df_state_forest = get_forest_cover_dataframe(state_geoId, level=\"County\")\n",
        "\n",
        "    if df_state_forest.empty:\n",
        "        print(f\"No forest data found for {state_geoId}. Skipping.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    years_to_include = range(start_year, end_year + 1)\n",
        "    all_years_data = pd.DataFrame()\n",
        "\n",
        "    # Filter for the desired years and concatenate\n",
        "    for year in years_to_include:\n",
        "        df_year = df_state_forest[df_state_forest[\"date\"].astype(int) == year].copy()\n",
        "\n",
        "        if not df_year.empty:\n",
        "            all_years_data = pd.concat([all_years_data, df_year], ignore_index=True)\n",
        "            print(f\"Added data for year {year}. Current shape: {all_years_data.shape}\")\n",
        "        else:\n",
        "            print(f\"No data found for year {year} for state {state_geoId}.\")\n",
        "\n",
        "    # Rename columns for clarity after combining\n",
        "    all_years_data = all_years_data.rename(columns={\n",
        "        \"entity\": \"Fips\",\n",
        "        \"entity_name\": \"county_name\",\n",
        "        \"date\": \"year\",\n",
        "        \"value\": \"Forest_Cover_Percent\"\n",
        "    })\n",
        "\n",
        "    # Add state geoId column\n",
        "    if not all_years_data.empty:\n",
        "         all_years_data['state_geoId'] = state_geoId\n",
        "\n",
        "\n",
        "    print(f\"\\nCombined forest cover data for {state_geoId} ({start_year}-{end_year}):\")\n",
        "    all_years_data.head()\n",
        "\n",
        "\n",
        "    return all_years_data\n",
        "\n",
        "# Example Usage:\n",
        "# Replace \"geoId/13\" with the geoId of the state you want to process\n",
        "state_to_process = \"geoId/36\" # Example: New York\n",
        "\n",
        "all_years_state_forest_data = get_all_years_forest_data_for_state(state_to_process, start_year=2015, end_year=2019)"
      ],
      "metadata": {
        "id": "J3zNn8InBBoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "from functools import reduce # Import reduce\n",
        "\n",
        "years_to_study = range(2015, 2020)\n",
        "yearly_forest_data = {}\n",
        "\n",
        "for year in years_to_study:\n",
        "    # Ensure 'year' is treated as integer for filtering\n",
        "    df_year = df_forest[df_forest[\"year\"].astype(int) == year].copy()\n",
        "    yearly_forest_data[year] = df_year\n",
        "    print(f\"Created dataframe for year {year} with shape: {df_year.shape}\")\n",
        "\n",
        "# You can access the dataframes using the dictionary, e.g., yearly_forest_data[2015]\n",
        "\n",
        "df_2015 = yearly_forest_data[2015]\n",
        "df_2016 = yearly_forest_data[2016]\n",
        "df_2017 = yearly_forest_data[2017]\n",
        "df_2018 = yearly_forest_data[2018]\n",
        "df_2019 = yearly_forest_data[2019]\n",
        "\n",
        "# List of dataframes to merge\n",
        "dfs_to_merge = [df_2015, df_2016, df_2017, df_2018, df_2019]\n",
        "\n",
        "# Define a function to merge two dataframes\n",
        "def merge_dfs(left, right):\n",
        "    # Extract year from the right dataframe's columns (assuming 'year' is still a column before renaming)\n",
        "    # Or, more reliably, use the year from the yearly_forest_data dictionary keys if available\n",
        "    # Let's assume we rename columns before merging to avoid conflicts and keep track of the year\n",
        "    year_right = right['year'].iloc[0] # Get the year from the 'year' column\n",
        "    right_renamed = right[['Fips', 'Forest_Cover_Percent']].rename(columns={'Forest_Cover_Percent': f'Forest_Cover_{year_right}'})\n",
        "\n",
        "    # Merge on 'Fips'\n",
        "    return pd.merge(left, right_renamed, on='Fips', how='outer')\n",
        "\n",
        "# Rename columns of the first dataframe before starting the merge\n",
        "df_2015_renamed = df_2015[['Fips', 'county_name', 'Forest_Cover_Percent']].rename(columns={'Forest_Cover_Percent': 'Forest_Cover_2015'})\n",
        "\n",
        "\n",
        "# Use reduce to apply the merge function sequentially\n",
        "# Start with the first renamed dataframe\n",
        "df_merged_all_years = reduce(merge_dfs, dfs_to_merge[1:], df_2015_renamed)\n",
        "\n",
        "print(\"\\nMerged DataFrame with forest cover for all years:\")\n",
        "df_merged_all_years.head()"
      ],
      "metadata": {
        "id": "oPzaEF0wC1Zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "from functools import reduce # Import reduce\n",
        "\n",
        "# Assuming get_forest_cover_dataframe is defined in a previous cell\n",
        "\n",
        "def get_merged_forest_data_for_state(state_geoId: str, start_year: int = 2015, end_year: int = 2019) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Fetches forest cover data for a single state across specified years,\n",
        "    merges it into a wide format DataFrame with one column per year,\n",
        "    and adds county_name and Fips columns.\n",
        "\n",
        "    Parameters:\n",
        "    - state_geoId: The geoId of the state (e.g., \"geoId/13\" for Georgia)\n",
        "    - start_year: The starting year (inclusive)\n",
        "    - end_year: The ending year (inclusive)\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame with forest cover data merged by year,\n",
        "      or an empty DataFrame if data fetching fails or no data is found.\n",
        "    \"\"\"\n",
        "    print(f\"\\nFetching forest data for state: {state_geoId} from {start_year} to {end_year} for merging\")\n",
        "\n",
        "    # Get forest cover data for the state's counties\n",
        "    df_state_forest = get_forest_cover_dataframe(state_geoId, level=\"County\")\n",
        "\n",
        "    if df_state_forest.empty:\n",
        "        print(f\"No forest data found for {state_geoId}. Skipping merge.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    years_to_include = range(start_year, end_year + 1)\n",
        "    dfs_to_merge = []\n",
        "    first_df = None\n",
        "\n",
        "    # Filter for the desired years and prepare for merging\n",
        "    for year in years_to_include:\n",
        "        df_year = df_state_forest[df_state_forest[\"date\"].astype(int) == year].copy()\n",
        "\n",
        "        if not df_year.empty:\n",
        "            # Rename value column to include year\n",
        "            df_year_renamed = df_year[['entity', 'entity_name', 'value']].rename(columns={\n",
        "                'entity': 'Fips',\n",
        "                'entity_name': 'county_name',\n",
        "                'value': f'Forest_Cover_{year}'\n",
        "            })\n",
        "            # Drop duplicates based on Fips to avoid issues during merge if any exist\n",
        "            df_year_renamed = df_year_renamed.drop_duplicates(subset=['Fips'])\n",
        "\n",
        "\n",
        "            if first_df is None:\n",
        "                first_df = df_year_renamed[['Fips', 'county_name', f'Forest_Cover_{year}']]\n",
        "            else:\n",
        "                dfs_to_merge.append(df_year_renamed[['Fips', f'Forest_Cover_{year}']])\n",
        "            print(f\"Prepared data for year {year}. Shape: {df_year_renamed.shape}\")\n",
        "        else:\n",
        "            print(f\"No data found for year {year} for state {state_geoId}. Skipping.\")\n",
        "\n",
        "    if first_df is None:\n",
        "        print(f\"No data available for any year in the range for {state_geoId}. Cannot merge.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "\n",
        "    # Define a function to merge two dataframes on 'Fips'\n",
        "    def merge_dfs_on_fips(left, right):\n",
        "        return pd.merge(left, right, on='Fips', how='outer')\n",
        "\n",
        "    # Use reduce to apply the merge function sequentially\n",
        "    # Start with the first dataframe\n",
        "    df_merged_all_years = reduce(merge_dfs_on_fips, dfs_to_merge, first_df)\n",
        "\n",
        "    print(f\"\\nMerged DataFrame for state {state_geoId} with forest cover for all years ({start_year}-{end_year}):\")\n",
        "    display(df_merged_all_years.head())\n",
        "\n",
        "\n",
        "    return df_merged_all_years\n",
        "\n",
        "# Example Usage (you can call this function later for different states):\n",
        "# state_geoId_to_process = \"geoId/13\" # Example: Georgia\n",
        "# df_merged_georgia = get_merged_forest_data_for_state(state_geoId_to_process, start_year=2015, end_year=2019)\n",
        "# if not df_merged_georgia.empty:\n",
        "#     print(f\"\\nShape of the merged dataframe for {state_geoId_to_process}: {df_merged_georgia.shape}\")\n",
        "# else:\n",
        "#     print(f\"Could not generate merged dataframe for {state_geoId_to_process}.\")"
      ],
      "metadata": {
        "id": "oWU9lo-BTuOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Usage with geoID/36 (New York)\n",
        "state_geoId_to_process = \"geoId/36\"\n",
        "df_merged_all_years = get_merged_forest_data_for_state(state_geoId_to_process, start_year=2015, end_year=2019)\n",
        "\n",
        "if not df_merged_all_years.empty:\n",
        "    print(f\"\\nShape of the merged dataframe for {state_geoId_to_process}: {df_merged_all_years.shape}\")\n",
        "    print(\"\\nMerged DataFrame head:\")\n",
        "    df_merged_all_years.head()\n",
        "else:\n",
        "    print(f\"Could not generate merged dataframe for {state_geoId_to_process}.\")"
      ],
      "metadata": {
        "id": "yPd8YiSSUSqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation for Analysis"
      ],
      "metadata": {
        "id": "2-1HkjOREDXn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dRS1lttZEZT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48eb0873"
      },
      "source": [
        "# Task\n",
        "Prepare the data from `df_merged_all_years` for machine learning analysis using EDA, feature sensitivity, and other related techniques, and provide required visualizations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2f6b92a"
      },
      "source": [
        "## Perform basic eda\n",
        "\n",
        "### Subtask:\n",
        "Display the head, info, and descriptive statistics of `df_merged_all_years` to understand its structure and content.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bda7c70d"
      },
      "source": [
        "**Reasoning**:\n",
        "Display the head, info, and descriptive statistics of the `df_merged_all_years` DataFrame to understand its structure and content as requested by the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e02f52d0"
      },
      "source": [
        "display(df_merged_all_years.head())\n",
        "df_merged_all_years.info()\n",
        "display(df_merged_all_years.describe().T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f245d2e"
      },
      "source": [
        "## Analyze missing values\n",
        "\n",
        "### Subtask:\n",
        "Check for and visualize the distribution of missing values in the DataFrame.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56eb8ab7"
      },
      "source": [
        "**Reasoning**:\n",
        "Calculate and display the number of missing values per column in the DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f89d0800"
      },
      "source": [
        "missing_values = df_merged_all_years.isnull().sum()\n",
        "print(\"Missing values per column:\")\n",
        "print(missing_values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2bb2e75"
      },
      "source": [
        "## Handle missing values\n",
        "\n",
        "### Subtask:\n",
        "Implement a strategy to handle missing values, such as imputation or removal.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91d27823"
      },
      "source": [
        "print(\"Based on the previous analysis, no missing values were found in the df_merged_all_years DataFrame.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcd79ada"
      },
      "source": [
        "## Feature engineering (if necessary)\n",
        "\n",
        "### Subtask:\n",
        "Create new features from existing ones in `df_merged_all_years` that might be relevant for the analysis of declining tree canopy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c904fd9"
      },
      "source": [
        "**Reasoning**:\n",
        "Calculate new features based on the forest cover percentages across the years and display the updated DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27e57698"
      },
      "source": [
        "# 1. Calculate total forest cover change from 2015 to 2019\n",
        "df_merged_all_years['Forest_Cover_Change_2015_2019'] = df_merged_all_years['Forest_Cover_2019'] - df_merged_all_years['Forest_Cover_2015']\n",
        "\n",
        "# 2. Calculate the average forest cover across all years (2015-2019)\n",
        "forest_cover_columns = ['Forest_Cover_2015', 'Forest_Cover_2016', 'Forest_Cover_2017', 'Forest_Cover_2018', 'Forest_Cover_2019']\n",
        "df_merged_all_years['Average_Forest_Cover'] = df_merged_all_years[forest_cover_columns].mean(axis=1)\n",
        "\n",
        "# 3. Calculate the standard deviation of forest cover across all years (2015-2019)\n",
        "df_merged_all_years['Forest_Cover_Std_Dev'] = df_merged_all_years[forest_cover_columns].std(axis=1)\n",
        "\n",
        "# 4. Display the head of the updated DataFrame\n",
        "df_merged_all_years.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2830cc79"
      },
      "source": [
        "## Target variable analysis\n",
        "\n",
        "### Subtask:\n",
        "Analyze the distribution and characteristics of the target variable (`Forest_Cover_Change_2015_2019`) in `df_merged_all_years`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c7ec6ed"
      },
      "source": [
        "**Reasoning**:\n",
        "Display the data type and basic descriptive statistics of the target variable, then plot its distribution and calculate skewness and kurtosis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2644ec48"
      },
      "source": [
        "# 1. Display data type and basic descriptive statistics\n",
        "print(\"Data type of 'Forest_Cover_Change_2015_2019':\", df_merged_all_years['Forest_Cover_Change_2015_2019'].dtype)\n",
        "print(\"\\nDescriptive Statistics of 'Forest_Cover_Change_2015_2019':\")\n",
        "display(df_merged_all_years['Forest_Cover_Change_2015_2019'].describe())\n",
        "\n",
        "# 2. Plot a histogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(df_merged_all_years['Forest_Cover_Change_2015_2019'], bins=30, color='skyblue', edgecolor='black')\n",
        "plt.title('Distribution of Forest Cover Change (2015-2019)')\n",
        "plt.xlabel('Forest Cover Change (%)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.show()\n",
        "\n",
        "# 3. Calculate and print skewness and kurtosis\n",
        "skewness = df_merged_all_years['Forest_Cover_Change_2015_2019'].skew()\n",
        "kurtosis = df_merged_all_years['Forest_Cover_Change_2015_2019'].kurtosis()\n",
        "print(f\"\\nSkewness of 'Forest_Cover_Change_2015_2019': {skewness:.4f}\")\n",
        "print(f\"Kurtosis of 'Forest_Cover_Change_2015_2019': {kurtosis:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65c610ae"
      },
      "source": [
        "## Perform feature sensitivity/correlation analysis\n",
        "\n",
        "### Subtask:\n",
        "Analyze the relationships between features and the target variable, and among features themselves, using correlation matrices. Visualize these relationships.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "557bd00a"
      },
      "source": [
        "**Reasoning**:\n",
        "Select numerical columns, calculate the correlation matrix, and visualize it using a heatmap to understand relationships among features and with the target variable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c6c8b37"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the `numpy` library was not imported. Import `numpy` and rerun the code to select numerical columns, calculate and visualize the correlation matrix, and analyze correlations with the target variable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySVU6eWlGBUp"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Select numerical columns, excluding 'Fips' and 'county_name'\n",
        "numerical_df = df_merged_all_years.select_dtypes(include=np.number)\n",
        "\n",
        "# 2. Calculate the Pearson correlation matrix\n",
        "correlation_matrix = numerical_df.corr()\n",
        "\n",
        "# 3. Generate a heatmap of the correlation matrix\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True)\n",
        "plt.title('Correlation Matrix of Numerical Features')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 4. Calculate the correlation between each numerical feature and the target variable\n",
        "target_correlation = correlation_matrix['Forest_Cover_Change_2015_2019'].sort_values(ascending=False)\n",
        "\n",
        "# 5. Display the correlation coefficients with the target\n",
        "print(\"\\nCorrelation with 'Forest_Cover_Change_2015_2019':\")\n",
        "display(target_correlation)\n",
        "\n",
        "# 6. Generate a bar plot for top N features' correlation with the target\n",
        "# Excluding the target variable itself from the plot\n",
        "target_correlation_for_plot = target_correlation.drop('Forest_Cover_Change_2015_2019')\n",
        "top_n = 10  # Choose N\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=target_correlation_for_plot.head(top_n).values,\n",
        "            y=target_correlation_for_plot.head(top_n).index,\n",
        "            palette='viridis',\n",
        "            hue=target_correlation_for_plot.head(top_n).index,\n",
        "            legend=False)\n",
        "plt.title(f'Top {top_n} Features Correlated with Forest Cover Change (2015-2019)')\n",
        "plt.xlabel('Pearson Correlation Coefficient')\n",
        "plt.ylabel('Feature')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=target_correlation_for_plot.tail(top_n).values,\n",
        "            y=target_correlation_for_plot.tail(top_n).index,\n",
        "            palette='magma',\n",
        "            hue=target_correlation_for_plot.tail(top_n).index,\n",
        "            legend=False)\n",
        "plt.title(f'Bottom {top_n} Features Correlated with Forest Cover Change (2015-2019)')\n",
        "plt.xlabel('Pearson Correlation Coefficient')\n",
        "plt.ylabel('Feature')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37f99ae2"
      },
      "source": [
        "## Visualize feature distributions\n",
        "\n",
        "### Subtask:\n",
        "Plot histograms for a selection of numerical features in `df_merged_all_years` to visualize their distributions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d7cf6ca"
      },
      "source": [
        "**Reasoning**:\n",
        "Select the numerical columns for plotting histograms and then iterate through them to create and display each histogram.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0f8d73f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Select numerical columns, excluding identifier columns\n",
        "numerical_columns_for_hist = [col for col in df_merged_all_years.select_dtypes(include=np.number).columns if col not in ['Fips', 'county_name', 'Forest_Cover_Change_2015_2019', 'Average_Forest_Cover', 'Forest_Cover_Std_Dev']]\n",
        "\n",
        "# Iterate through selected columns and plot histograms\n",
        "for col in numerical_columns_for_hist:\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.hist(df_merged_all_years[col], bins=30, color='teal', edgecolor='black')\n",
        "    plt.title(f'Distribution of {col}')\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.grid(axis='y', alpha=0.75)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b33d7e3"
      },
      "source": [
        "## Split data\n",
        "\n",
        "### Subtask:\n",
        "Split the data into training and testing sets for model building and evaluation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89a6848a"
      },
      "source": [
        "**Reasoning**:\n",
        "Define features and target, then split the data into training and testing sets and print the shapes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a60499a"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because 'Fips' was not in the numerical columns after selecting only numeric types. Need to drop 'Fips' and 'county_name' from the original dataframe before selecting numerical columns.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp3zQtELGW5P"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Drop non-numerical and target columns before selecting numerical types\n",
        "features_df = df_merged_all_years.drop(columns=['Fips', 'county_name', 'Forest_Cover_Change_2015_2019'])\n",
        "\n",
        "# Define features (X) by selecting numerical columns\n",
        "X = features_df.select_dtypes(include=np.number)\n",
        "\n",
        "# Define target (y)\n",
        "y = df_merged_all_years['Forest_Cover_Change_2015_2019']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.2,  # 20% for testing\n",
        "    random_state=42 # for reproducibility\n",
        ")\n",
        "\n",
        "# Print the shapes of the resulting splits\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26f8613d"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The initial EDA revealed that the `df_merged_all_years` DataFrame contains 159 entries and no missing values across all columns.\n",
        "*   On average, there was a decrease in forest cover between 2015 and 2019, with the `Forest_Cover_Change_2015_2019` having a mean of -2.78. The change ranged from a decrease of approximately 7.83% to an increase of approximately 1.53%.\n",
        "*   The distribution of `Forest_Cover_Change_2015_2019` is slightly left-skewed (skewness: -0.2639) and slightly platykurtic (kurtosis: -0.4016).\n",
        "*   Features like `Forest_Cover_2015`, `Forest_Cover_Std_Dev`, and forest cover in other years (`2016` to `2019`) show strong negative correlations with the `Forest_Cover_Change_2015_2019`.\n",
        "*   The data was successfully split into training (80%) and testing (20%) sets, with the shapes of the resulting sets printed for verification.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The strong negative correlation between initial forest cover (`Forest_Cover_2015`) and the change in forest cover suggests that areas with higher initial forest cover might have experienced larger absolute declines. This could be a key factor to explore further in modeling.\n",
        "*   The absence of missing values simplifies the initial data preparation. The engineered features (`Forest_Cover_Change_2015_2019`, `Average_Forest_Cover`, `Forest_Cover_Std_Dev`) capture different aspects of forest cover dynamics and should be valuable for predictive modeling.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Machine Learning on Data"
      ],
      "metadata": {
        "id": "W6CY12aWNHji"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5de8656"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming df_merged_all_years is available from previous steps\n",
        "\n",
        "# Define thresholds for categorization (these can be adjusted)\n",
        "# For example, classify based on quantiles or domain knowledge\n",
        "change_column = df_merged_all_years['Forest_Cover_Change_2015_2019']\n",
        "\n",
        "# Example categorization based on ranges:\n",
        "# Significant Loss: e.g., change < -4%\n",
        "# Slight Loss/Stable: e.g., -4% <= change <= 1%\n",
        "# Gain: e.g., change > 1%\n",
        "\n",
        "# Let's determine some example thresholds based on the distribution\n",
        "# Using percentiles can be a robust way if specific domain thresholds aren't defined\n",
        "loss_threshold = change_column.quantile(0.25) # e.g., counties in the bottom 25% of change\n",
        "gain_threshold = change_column.quantile(0.75) # e.g., counties in the top 25% of change\n",
        "\n",
        "print(f\"Using thresholds based on quantiles: Loss < {loss_threshold:.2f}%, Gain > {gain_threshold:.2f}%\")\n",
        "\n",
        "\n",
        "def categorize_change(change, loss_thresh, gain_thresh):\n",
        "    if change < loss_thresh:\n",
        "        return 'Significant Loss'\n",
        "    elif change > gain_thresh:\n",
        "        return 'Gain'\n",
        "    else:\n",
        "        return 'Slight Loss/Stable'\n",
        "\n",
        "# Apply the categorization function\n",
        "df_merged_all_years['Canopy_Change_Category'] = df_merged_all_years['Forest_Cover_Change_2015_2019'].apply(\n",
        "    lambda x: categorize_change(x, loss_threshold, gain_threshold)\n",
        ")\n",
        "\n",
        "# Display the counts for each category\n",
        "print(\"\\nCounts of counties in each canopy change category:\")\n",
        "display(df_merged_all_years['Canopy_Change_Category'].value_counts())\n",
        "\n",
        "# Display the DataFrame with the new category column\n",
        "print(\"\\nDataFrame head with Canopy_Change_Category:\")\n",
        "display(df_merged_all_years.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming df_merged_all_years DataFrame with 'Forest_Cover_Change_2015_2019' and 'county_name' exists\n",
        "\n",
        "# Sort the DataFrame by the forest cover change to highlight largest drops/gains\n",
        "df_sorted_change = df_merged_all_years.sort_values(by='Forest_Cover_Change_2015_2019', ascending=False)\n",
        "\n",
        "# Select a reasonable number of top and bottom counties to visualize if there are many\n",
        "# For simplicity, let's visualize all if the number of counties is manageable, or top/bottom N otherwise\n",
        "# Let's set a threshold, e.g., if more than 50 counties, show top/bottom 25\n",
        "num_counties = len(df_sorted_change)\n",
        "if num_counties > 50:\n",
        "    top_n = 25\n",
        "    bottom_n = 25\n",
        "    df_viz = pd.concat([df_sorted_change.head(top_n), df_sorted_change.tail(bottom_n)])\n",
        "else:\n",
        "    df_viz = df_sorted_change\n",
        "\n",
        "# Create the bar chart\n",
        "plt.figure(figsize=(12, max(6, len(df_viz) * 0.3))) # Adjust figure size based on number of counties\n",
        "sns.barplot(data=df_viz, x='Forest_Cover_Change_2015_2019', y='county_name', palette='coolwarm', hue='county_name', legend=False)\n",
        "\n",
        "plt.title('Forest Cover Change (2015-2019) by County')\n",
        "plt.xlabel('Forest Cover Change (%)')\n",
        "plt.ylabel('County Name')\n",
        "plt.grid(axis='x', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cdPFDEC2WrO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attempts at Logistic Regression Based on Available Data (Canopy) - Savar"
      ],
      "metadata": {
        "id": "wXzH7LhDYnfS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad135f00"
      },
      "source": [
        "## Reshape data\n",
        "\n",
        "### Subtask:\n",
        "Melt the `df_merged_all_years` DataFrame to transform the yearly forest cover columns into a single 'Year' column and a 'Forest_Cover' column.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71566835"
      },
      "source": [
        "**Reasoning**:\n",
        "Melt the DataFrame to transform the yearly forest cover columns into a long format as requested by the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43d53730"
      },
      "source": [
        "# Melt the DataFrame to long format\n",
        "df_long = pd.melt(\n",
        "    df_merged_all_years,\n",
        "    id_vars=['Fips', 'county_name', 'Forest_Cover_Change_2015_2019', 'Average_Forest_Cover', 'Forest_Cover_Std_Dev', 'Canopy_Change_Category'],\n",
        "    var_name='Year',\n",
        "    value_name='Forest_Cover'\n",
        ")\n",
        "\n",
        "# Display the head of the long-format DataFrame to verify\n",
        "df_long.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "159ed326"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because `df_merged_all_years` was not defined. I need to re-run the code that creates `df_merged_all_years` before melting it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7d740d3"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from statsmodels.tsa.holtwinters import Holt\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Ensure 'Year' is treated as a numerical feature for modeling\n",
        "# Extract the year number from the string and convert to integer\n",
        "df_long['Year'] = df_long['Year'].str.extract(r'_(\\d{4})').astype(int)\n",
        "\n",
        "# For time series modeling, it's often useful to have a single time series.\n",
        "# Let's aggregate the data to get the average forest cover per year across all counties.\n",
        "# This simplifies the forecasting for demonstration purposes.\n",
        "# If county-level forecasts are needed, the approach would be different (e.g., loop through counties or use a hierarchical model).\n",
        "average_yearly_forest_cover = df_long.groupby('Year')['Forest_Cover'].mean().reset_index()\n",
        "\n",
        "# Ensure the index is the year for time series models\n",
        "average_yearly_forest_cover = average_yearly_forest_cover.set_index('Year')\n",
        "\n",
        "print(\"Average yearly forest cover data prepared for modeling:\")\n",
        "display(average_yearly_forest_cover.head())\n",
        "\n",
        "# Prepare data for linear regression\n",
        "X_lr = average_yearly_forest_cover.index.values.reshape(-1, 1) # Years as feature\n",
        "y_lr = average_yearly_forest_cover['Forest_Cover'].values       # Forest Cover as target\n",
        "\n",
        "# Prepare data for Holt's Exponential Smoothing\n",
        "# Holt's model works directly on the time series data (y_lr in this case)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try Basic Regression and Holt Smoothing"
      ],
      "metadata": {
        "id": "2WMkbLjLgDql"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41fd12e8"
      },
      "source": [
        "# Fit the Linear Regression model\n",
        "linear_model = LinearRegression()\n",
        "linear_model.fit(X_lr, y_lr)\n",
        "\n",
        "print(\"Linear Regression model fitted successfully.\")\n",
        "print(f\"Intercept: {linear_model.intercept_:.4f}\")\n",
        "print(f\"Coefficient: {linear_model.coef_[0]:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ab664a8"
      },
      "source": [
        "# Forecast for the next 5 years using the linear model\n",
        "last_year = average_yearly_forest_cover.index.max()\n",
        "future_years = np.arange(last_year + 1, last_year + 6).reshape(-1, 1)\n",
        "linear_forecast = linear_model.predict(future_years)\n",
        "\n",
        "print(\"Linear regression forecast for the next 5 years:\")\n",
        "for year, forecast in zip(future_years.flatten(), linear_forecast):\n",
        "    print(f\"Year {year}: {forecast:.4f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99a7002d"
      },
      "source": [
        "from statsmodels.tsa.holtwinters import Holt\n",
        "import pandas as pd\n",
        "\n",
        "# Fit Holt's Exponential Smoothing model\n",
        "# The Holt's method is for data with a trend.\n",
        "# We use the 'add' trend type as the trend appears additive.\n",
        "\n",
        "# Create a time series object with a DatetimeIndex, as expected by statsmodels\n",
        "# The data is yearly, so a frequency of 'AS' (Annual Start) is appropriate.\n",
        "time_series_data = pd.Series(\n",
        "    average_yearly_forest_cover['Forest_Cover'].values,\n",
        "    index=pd.to_datetime(average_yearly_forest_cover.index, format='%Y')\n",
        ")\n",
        "time_series_data.index.freq = 'AS' # Set the frequency\n",
        "\n",
        "holt_model = Holt(time_series_data).fit(smoothing_level = 0.3, smoothing_trend = 0.1) # Parameters can be tuned\n",
        "\n",
        "print(\"\\nHolt's Exponential Smoothing model fitted successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb0cb9a1"
      },
      "source": [
        "# Forecast for the next 5 years using the Holt's model\n",
        "# The forecast method needs the number of steps to forecast\n",
        "holt_forecast = holt_model.forecast(steps=5)\n",
        "\n",
        "print(\"\\nHolt's Exponential Smoothing forecast for the next 5 years:\")\n",
        "display(holt_forecast)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3a10c89"
      },
      "source": [
        "# Visualize the historical data and the fitted/forecasted lines from both models\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "\n",
        "# Plot historical data\n",
        "plt.plot(average_yearly_forest_cover.index, average_yearly_forest_cover['Forest_Cover'], marker='o', linestyle='-', color='blue', label='Historical Data (2015-2019)')\n",
        "\n",
        "# Create years for plotting fitted lines (2015 to 2024)\n",
        "years_for_plotting = np.arange(average_yearly_forest_cover.index.min(), future_years.max() + 1).reshape(-1, 1)\n",
        "\n",
        "# Plot Linear Regression fitted and forecasted line\n",
        "linear_fitted_and_forecast = linear_model.predict(years_for_plotting)\n",
        "plt.plot(years_for_plotting.flatten(), linear_fitted_and_forecast, linestyle='--', color='red', label='Linear Regression Fit & Forecast (2015-2024)')\n",
        "\n",
        "# Plot Holt's Exponential Smoothing fitted and forecasted line\n",
        "# Get fitted values for the historical period and concatenate with the forecast\n",
        "holt_fitted = holt_model.fittedvalues\n",
        "holt_fitted_and_forecast_index = holt_fitted.index.union(holt_forecast.index)\n",
        "holt_fitted_and_forecast_values = np.concatenate([holt_fitted.values, holt_forecast.values])\n",
        "\n",
        "# Align the combined fitted and forecasted values with the full range of years for plotting\n",
        "holt_plot_series = pd.Series(holt_fitted_and_forecast_values, index=holt_fitted_and_forecast_index).reindex(pd.to_datetime(years_for_plotting.flatten(), format='%Y'))\n",
        "\n",
        "\n",
        "plt.plot(holt_plot_series.index.year, holt_plot_series.values, linestyle='--', color='green', label=\"Holt's Exponential Smoothing Fit & Forecast (2015-2024)\")\n",
        "\n",
        "\n",
        "plt.title('Forest Canopy Cover Forecast (2015-2024) with Model Fits')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Average Forest Cover (%)')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.xticks(np.arange(average_yearly_forest_cover.index.min(), future_years.max() + 1, 1)) # Ensure all years are shown on x-axis\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Polynomial Regression"
      ],
      "metadata": {
        "id": "2_9CMk5qgUIi"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a00e133"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming average_yearly_forest_cover is available from previous steps\n",
        "# Ensure Year is the index and Forest_Cover is the value\n",
        "# Check if the index is already integer type, if not convert it\n",
        "if not pd.api.types.is_integer_dtype(average_yearly_forest_cover.index):\n",
        "     average_yearly_forest_cover.index = average_yearly_forest_cover.index.astype(int)\n",
        "\n",
        "# Prepare data for modeling\n",
        "# X should be the years, y should be the forest cover\n",
        "X_hist = average_yearly_forest_cover.index.values.reshape(-1, 1)\n",
        "y_hist = average_yearly_forest_cover['Forest_Cover'].values\n",
        "\n",
        "# Define years for forecasting (next 5 years)\n",
        "last_year = average_yearly_forest_cover.index.max()\n",
        "future_years_arr = np.arange(last_year + 1, last_year + 6).reshape(-1, 1)\n",
        "\n",
        "# Combine historical and future years for plotting\n",
        "all_years_arr = np.arange(average_yearly_forest_cover.index.min(), future_years_arr.max() + 1).reshape(-1, 1)\n",
        "\n",
        "# --- Fit Polynomial Models ---\n",
        "\n",
        "# Linear Model (Degree 1)\n",
        "linear_model = make_pipeline(PolynomialFeatures(1), LinearRegression())\n",
        "linear_model.fit(X_hist, y_hist)\n",
        "linear_forecast_all_years = linear_model.predict(all_years_arr)\n",
        "\n",
        "# Quadratic Model (Degree 2)\n",
        "quadratic_model = make_pipeline(PolynomialFeatures(2), LinearRegression())\n",
        "quadratic_model.fit(X_hist, y_hist)\n",
        "quadratic_forecast_all_years = quadratic_model.predict(all_years_arr)\n",
        "\n",
        "\n",
        "# Cubic Model (Degree 3)\n",
        "cubic_model = make_pipeline(PolynomialFeatures(3), LinearRegression())\n",
        "cubic_model.fit(X_hist, y_hist)\n",
        "cubic_forecast_all_years = cubic_model.predict(all_years_arr)\n",
        "\n",
        "\n",
        "print(\"Polynomial models fitted successfully.\")\n",
        "\n",
        "# --- Visualize Results ---\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "\n",
        "# Plot historical data\n",
        "plt.plot(average_yearly_forest_cover.index, average_yearly_forest_cover['Forest_Cover'], marker='o', linestyle='-', color='blue', label='Historical Data (2015-2019)')\n",
        "\n",
        "# Plot fitted and forecasted lines for each model\n",
        "plt.plot(all_years_arr.flatten(), linear_forecast_all_years, linestyle='--', color='red', label='Linear Fit & Forecast')\n",
        "plt.plot(all_years_arr.flatten(), quadratic_forecast_all_years, linestyle='--', color='purple', label='Quadratic Fit & Forecast')\n",
        "plt.plot(all_years_arr.flatten(), cubic_forecast_all_years, linestyle='--', color='orange', label='Cubic Fit & Forecast')\n",
        "\n",
        "\n",
        "plt.title('Forest Canopy Cover Forecast (2015-2024) with Polynomial Fits')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Average Forest Cover (%)')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.xticks(all_years_arr.flatten()) # Ensure all years are shown on x-axis\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ARIMA Time Series Specific Regression"
      ],
      "metadata": {
        "id": "1ritnL1TgbMA"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e60b8e2"
      },
      "source": [
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "# Assuming average_yearly_forest_cover is available from previous steps\n",
        "ts_data = average_yearly_forest_cover['Forest_Cover']\n",
        "\n",
        "# Perform Augmented Dickey-Fuller test\n",
        "result = adfuller(ts_data)\n",
        "\n",
        "print('ADF Statistic: %f' % result[0])\n",
        "print('p-value: %f' % result[1])\n",
        "print('Critical Values:')\n",
        "for key, value in result[4].items():\n",
        "    print('\\t%s: %.3f' % (key, value))\n",
        "\n",
        "# Interpret the result\n",
        "if result[1] <= 0.05:\n",
        "    print(\"\\nResult: The time series is likely stationary (reject H0)\")\n",
        "else:\n",
        "    print(\"\\nResult: The time series is likely non-stationary (fail to reject H0)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fae1464"
      },
      "source": [
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming time_series_data is available from the previous step and has a DatetimeIndex\n",
        "\n",
        "# Fit a simple ARIMA(1, 1, 0) model\n",
        "# (p=1: autoregressive order, d=1: differencing order, q=0: moving average order)\n",
        "# The 'enforce_stationarity' and 'enforce_invertibility' are set to False\n",
        "# to allow fitting even with limited data points, but results should be interpreted with caution.\n",
        "try:\n",
        "    model = ARIMA(time_series_data, order=(1, 1, 0))\n",
        "    arima_result = model.fit()\n",
        "    print(arima_result.summary())\n",
        "except Exception as e:\n",
        "    print(f\"Error fitting ARIMA model: {e}\")\n",
        "    print(\"It might be challenging to fit even a simple ARIMA model with only 5 data points.\")\n",
        "    arima_result = None # Ensure arima_result is None if fitting fails"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fb4e82d"
      },
      "source": [
        "# Forecast for the next 5 years using the fitted ARIMA model\n",
        "if arima_result:\n",
        "    arima_forecast = arima_result.forecast(steps=5)\n",
        "\n",
        "    print(\"\\nARIMA Forecast for the next 5 years:\")\n",
        "    display(arima_forecast)\n",
        "else:\n",
        "    print(\"\\nARIMA model fitting failed, cannot generate forecast.\")\n",
        "    arima_forecast = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c710ef8"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming average_yearly_forest_cover and arima_forecast are available\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "\n",
        "# Plot historical data\n",
        "plt.plot(average_yearly_forest_cover.index, average_yearly_forest_cover['Forest_Cover'], marker='o', linestyle='-', color='blue', label='Historical Data (2015-2019)')\n",
        "\n",
        "# Plot ARIMA forecast\n",
        "if arima_forecast is not None:\n",
        "    # The index of arima_forecast is already in DatetimeIndex format, convert to year for plotting\n",
        "    plt.plot(arima_forecast.index.year, arima_forecast.values, marker='o', linestyle='--', color='purple', label='ARIMA Forecast (2020-2024)')\n",
        "\n",
        "plt.title('Forest Canopy Cover Historical Data and ARIMA Forecast')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Average Forest Cover (%)')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "# Set x-ticks to include all years from historical data start to forecast end\n",
        "all_years = pd.concat([average_yearly_forest_cover.reset_index()['Year'], pd.Series(arima_forecast.index.year)]).unique()\n",
        "plt.xticks(all_years)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating end-to-end pipeline from state to plots - Savar"
      ],
      "metadata": {
        "id": "ZRzra-Mr3tNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Melt the DataFrame to long format\n",
        "df_long = pd.melt(\n",
        "    df_merged_all_years,\n",
        "    id_vars=['Fips', 'county_name', 'Forest_Cover_Change_2015_2019', 'Average_Forest_Cover', 'Forest_Cover_Std_Dev', 'Canopy_Change_Category'],\n",
        "    var_name='Year',\n",
        "    value_name='Forest_Cover'\n",
        ")\n",
        "\n",
        "# Extract numeric year from strings like \"Forest_Cover_2015\"\n",
        "df_long[\"Year\"] = df_long[\"Year\"].str.extract(r'(\\d{4})').astype(int)\n",
        "\n",
        "# Ensure Forest_Cover is numeric\n",
        "df_long[\"Forest_Cover\"] = pd.to_numeric(df_long[\"Forest_Cover\"], errors=\"coerce\")\n",
        "\n",
        "# Preview\n",
        "display(df_long.head())"
      ],
      "metadata": {
        "id": "LI7VMvbeA3i5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "\n",
        "# Ensure Year is numeric\n",
        "df_long[\"Year\"] = df_long[\"Year\"].astype(int)\n",
        "\n",
        "# Container for results\n",
        "all_results = []\n",
        "\n",
        "# Forecast horizon (e.g., 10 years ahead)\n",
        "horizon = 10\n",
        "future_years_template = lambda last_year: np.arange(last_year+1, last_year+horizon+1)\n",
        "\n",
        "# Loop over each county (keep FIPS too)\n",
        "for (fips, county), grp in df_long.groupby([\"Fips\", \"county_name\"]):\n",
        "    grp = grp.sort_values(\"Year\")\n",
        "    X = grp[\"Year\"].values.reshape(-1, 1)\n",
        "    y = grp[\"Forest_Cover\"].values\n",
        "\n",
        "    if len(grp) < 3:\n",
        "        continue  # skip counties with too few data points\n",
        "\n",
        "    # ---- Linear Regression ----\n",
        "    lin_model = LinearRegression()\n",
        "    lin_model.fit(X, y)\n",
        "\n",
        "    # Future predictions\n",
        "    future_years = future_years_template(grp[\"Year\"].max()).reshape(-1,1)\n",
        "    lin_future_preds = lin_model.predict(future_years)\n",
        "\n",
        "    # ---- Holt’s Exponential Smoothing ----\n",
        "    try:\n",
        "        holt_model = ExponentialSmoothing(y, trend=\"add\", seasonal=None).fit()\n",
        "        holt_future_preds = holt_model.forecast(horizon)\n",
        "    except:\n",
        "        holt_future_preds = [np.nan]*horizon\n",
        "\n",
        "    # ---- Historical part (observed fills forecasts) ----\n",
        "    hist_df = pd.DataFrame({\n",
        "        \"Fips\": fips,\n",
        "        \"county_name\": county,\n",
        "        \"Year\": grp[\"Year\"].values,\n",
        "        \"Linear_Forecast\": y,   # observed values\n",
        "        \"Holt_Forecast\": y      # observed values\n",
        "    })\n",
        "\n",
        "    # ---- Future forecasts ----\n",
        "    future_df = pd.DataFrame({\n",
        "        \"Fips\": fips,\n",
        "        \"county_name\": county,\n",
        "        \"Year\": future_years.flatten(),\n",
        "        \"Linear_Forecast\": lin_future_preds,\n",
        "        \"Holt_Forecast\": holt_future_preds\n",
        "    })\n",
        "\n",
        "    # Combine\n",
        "    all_results.append(pd.concat([hist_df, future_df], ignore_index=True))\n",
        "\n",
        "# Final dataset with 2015 → latest year + 10yr forecast\n",
        "all_forecasts = pd.concat(all_results, ignore_index=True)\n",
        "\n",
        "# Ensure only years >= 2015\n",
        "all_forecasts = all_forecasts[all_forecasts[\"Year\"] >= 2015]\n",
        "\n",
        "# Preview\n",
        "display(all_forecasts.head(20))\n"
      ],
      "metadata": {
        "id": "Iv7W5do5LWt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pivot Linear forecasts\n",
        "linear_pivot = all_forecasts.pivot(\n",
        "    index=[\"Fips\", \"county_name\"],\n",
        "    columns=\"Year\",\n",
        "    values=\"Linear_Forecast\"\n",
        ").add_prefix(\"Linear_\")\n",
        "\n",
        "# Pivot Holt forecasts\n",
        "holt_pivot = all_forecasts.pivot(\n",
        "    index=[\"Fips\", \"county_name\"],\n",
        "    columns=\"Year\",\n",
        "    values=\"Holt_Forecast\"\n",
        ").add_prefix(\"Holt_\")\n",
        "\n",
        "# Merge side by side\n",
        "forecasts_wide = pd.concat([linear_pivot, holt_pivot], axis=1).reset_index()\n",
        "\n",
        "# Optional: sort columns for readability\n",
        "non_year_cols = [\"Fips\", \"county_name\"]\n",
        "year_cols = sorted([c for c in forecasts_wide.columns if c not in non_year_cols])\n",
        "forecasts_wide = forecasts_wide.loc[:, non_year_cols + year_cols]\n",
        "\n",
        "display(forecasts_wide.head())"
      ],
      "metadata": {
        "id": "mBM8bR6WNw2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "\n",
        "# Make sure Year is int\n",
        "all_forecasts[\"Year\"] = all_forecasts[\"Year\"].astype(int)\n",
        "\n",
        "# Also grab observed data from df_long for plotting\n",
        "observed = df_long.copy()\n",
        "observed[\"Year\"] = observed[\"Year\"].astype(int)\n",
        "\n",
        "# List of counties\n",
        "counties = sorted(all_forecasts[\"county_name\"].unique())\n",
        "\n",
        "# Create initial figure for the first county\n",
        "init_county = counties[0]\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Observed\n",
        "obs_grp = observed[observed[\"county_name\"] == init_county]\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=obs_grp[\"Year\"], y=obs_grp[\"Forest_Cover\"],\n",
        "    mode=\"markers+lines\", name=\"Observed\", marker=dict(color=\"black\")\n",
        "))\n",
        "\n",
        "# Linear forecast\n",
        "lin_grp = all_forecasts[all_forecasts[\"county_name\"] == init_county]\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=lin_grp[\"Year\"], y=lin_grp[\"Linear_Forecast\"],\n",
        "    mode=\"lines\", name=\"Linear Forecast\", line=dict(dash=\"dash\", color=\"red\")\n",
        "))\n",
        "\n",
        "# Holt forecast\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=lin_grp[\"Year\"], y=lin_grp[\"Holt_Forecast\"],\n",
        "    mode=\"lines\", name=\"Holt Forecast\", line=dict(color=\"blue\")\n",
        "))\n",
        "\n",
        "# Dropdown menu: one button per county\n",
        "dropdown_buttons = []\n",
        "for county in counties:\n",
        "    obs_grp = observed[observed[\"county_name\"] == county]\n",
        "    lin_grp = all_forecasts[all_forecasts[\"county_name\"] == county]\n",
        "\n",
        "    dropdown_buttons.append(\n",
        "        dict(\n",
        "            method=\"update\",\n",
        "            label=county,\n",
        "            args=[\n",
        "                {\"x\": [obs_grp[\"Year\"], lin_grp[\"Year\"], lin_grp[\"Year\"]],\n",
        "                 \"y\": [obs_grp[\"Forest_Cover\"], lin_grp[\"Linear_Forecast\"], lin_grp[\"Holt_Forecast\"]]},\n",
        "                {\"title\": f\"Forest Cover Trends for {county}\"}\n",
        "            ]\n",
        "        )\n",
        "    )\n",
        "\n",
        "fig.update_layout(\n",
        "    title=f\"Forest Cover Trends for {init_county}\",\n",
        "    xaxis_title=\"Year\",\n",
        "    yaxis_title=\"Forest Cover (%)\",\n",
        "    updatemenus=[dict(\n",
        "        active=0,\n",
        "        buttons=dropdown_buttons,\n",
        "        x=1.15,\n",
        "        y=1,\n",
        "        xanchor=\"left\",\n",
        "        yanchor=\"top\"\n",
        "    )]\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "AiYfKZimO6Ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Prepare geoID (state identifier from FIPS) ---\n",
        "\n",
        "# Ensure FIPS is string (leading zeros preserved)\n",
        "df_long[\"Fips\"] = df_long[\"Fips\"].astype(str).str.zfill(5)\n",
        "\n",
        "# Create geoID column = first two digits of FIPS (state code)\n",
        "df_long[\"geoID\"] = df_long[\"Fips\"].str[:2]\n",
        "\n",
        "# Get all unique geoIDs available in dataset\n",
        "unique_geoIDs = df_long[\"geoID\"].unique()\n",
        "\n",
        "print(f\"Available geoIDs (states): {unique_geoIDs}\")\n",
        "print(f\"Total unique states found: {len(unique_geoIDs)}\")\n"
      ],
      "metadata": {
        "id": "uoEhqQF3FkNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modify Process by State as a Function"
      ],
      "metadata": {
        "id": "xkhJW9TsD2De"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_state(df_state: pd.DataFrame, geoID: str, horizon: int = 10):\n",
        "    \"\"\"\n",
        "    Performs EDA, Linear Regression, and ARIMA forecasts for all counties in a state.\n",
        "    Returns a combined forecast DataFrame and an interactive Plotly figure with dropdown.\n",
        "    \"\"\"\n",
        "    all_results = []\n",
        "\n",
        "    # --- Normalize column names ---\n",
        "    df_state = df_state.copy()\n",
        "    df_state.columns = [c.lower() for c in df_state.columns]\n",
        "\n",
        "    # Ensure correct types\n",
        "    df_state[\"year\"] = df_state[\"year\"].astype(int)\n",
        "    df_state = df_state.sort_values([\"fips\", \"year\"])\n",
        "\n",
        "    for fips, grp in df_state.groupby(\"fips\"):\n",
        "        county = grp[\"county_name\"].iloc[0]\n",
        "        X = grp[\"year\"].values.reshape(-1, 1)\n",
        "        y = grp[\"forest_cover_percent\"].values\n",
        "\n",
        "        if len(grp) < 3:\n",
        "            continue\n",
        "\n",
        "        # --- Linear Regression ---\n",
        "        lin_model = LinearRegression()\n",
        "        lin_model.fit(X, y)\n",
        "\n",
        "        future_years = np.arange(grp[\"year\"].max() + 1,\n",
        "                                 grp[\"year\"].max() + horizon + 1).reshape(-1, 1)\n",
        "        lin_future_preds = lin_model.predict(future_years)\n",
        "\n",
        "        # --- ARIMA ---\n",
        "        try:\n",
        "            arima_model = ARIMA(y, order=(1, 1, 1))\n",
        "            # Suppress ARIMA summary output by capturing it (or pass disp=False if available)\n",
        "            # Note: Some statsmodels versions might still print warnings or errors to stderr\n",
        "            with np.printoptions(threshold=np.inf): # Temporarily allow printing large arrays if any in output\n",
        "                arima_result = arima_model.fit(disp=False) # Use disp=False to suppress summary\n",
        "\n",
        "            arima_future_preds = arima_result.forecast(steps=horizon)\n",
        "        except Exception as e:\n",
        "            # print(f\"Could not fit ARIMA for {county} ({fips}): {e}\") # Optionally log failures without being verbose\n",
        "            arima_future_preds = [np.nan] * horizon # Fill with NaN on failure\n",
        "\n",
        "        # Historical values\n",
        "        hist_df = pd.DataFrame({\n",
        "            \"Fips\": fips,\n",
        "            \"county_name\": county,\n",
        "            \"year\": grp[\"year\"].values,\n",
        "            \"Linear_Forecast\": y,\n",
        "            \"ARIMA_Forecast\": y # Use observed for historical ARIMA points\n",
        "        })\n",
        "\n",
        "        # Future values\n",
        "        future_df = pd.DataFrame({\n",
        "            \"Fips\": fips,\n",
        "            \"county_name\": county,\n",
        "            \"year\": future_years.flatten(),\n",
        "            \"Linear_Forecast\": lin_future_preds,\n",
        "            \"ARIMA_Forecast\": arima_future_preds\n",
        "        })\n",
        "\n",
        "        all_results.append(pd.concat([hist_df, future_df], ignore_index=True))\n",
        "\n",
        "    if not all_results:\n",
        "        print(\"No counties had sufficient data for analysis.\")\n",
        "        return None, None\n",
        "\n",
        "    forecast_df = pd.concat(all_results, ignore_index=True)\n",
        "    forecast_df = forecast_df[forecast_df[\"year\"] >= 2015]\n",
        "\n",
        "    # --- Interactive Plot ---\n",
        "    fig = go.Figure()\n",
        "    county_options = forecast_df[\"county_name\"].unique()\n",
        "\n",
        "    for county in county_options:\n",
        "        df_c = forecast_df[forecast_df[\"county_name\"] == county]\n",
        "        # Filter out NaNs for plotting if ARIMA failed for a county\n",
        "        df_c_arima_valid = df_c.dropna(subset=[\"ARIMA_Forecast\"])\n",
        "\n",
        "        # Plot Observed (historical only) - Assuming observed data is needed for comparison\n",
        "        # Find the last historical year from the original data to split observed from forecast\n",
        "        last_historical_year = df_state[\"year\"].max()\n",
        "        df_c_observed = df_c[df_c[\"year\"] <= last_historical_year]\n",
        "\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=df_c_observed[\"year\"], y=df_c_observed[\"Linear_Forecast\"], # Linear_Forecast column holds observed for historical\n",
        "            mode=\"markers+lines\", name=f\"{county} - Observed\",\n",
        "            visible=(county == county_options[0]),\n",
        "            marker=dict(color=\"black\")\n",
        "        ))\n",
        "\n",
        "\n",
        "        # Plot Linear forecast (Historical + Future)\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=df_c[\"year\"], y=df_c[\"Linear_Forecast\"],\n",
        "            mode=\"lines\", name=f\"{county} - Linear\",\n",
        "            visible=(county == county_options[0]),\n",
        "            line=dict(dash=\"dash\", color=\"red\")\n",
        "        ))\n",
        "\n",
        "        # Plot ARIMA forecast (Historical + Future, excluding NaNs)\n",
        "        if not df_c_arima_valid.empty:\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=df_c_arima_valid[\"year\"], y=df_c_arima_valid[\"ARIMA_Forecast\"],\n",
        "                mode=\"lines\", name=f\"{county} - ARIMA\",\n",
        "                visible=(county == county_options[0]),\n",
        "                line=dict(color=\"blue\")\n",
        "            ))\n",
        "        # Else: No trace added if ARIMA failed for this county\n",
        "\n",
        "\n",
        "    # Create buttons dynamically based on the traces added\n",
        "    # Each county will have 3 traces: Observed, Linear, ARIMA (if ARIMA fit)\n",
        "    # Need to map county to the indices of its traces\n",
        "    county_trace_map = {}\n",
        "    current_trace_idx = 0\n",
        "    for county in county_options:\n",
        "        county_trace_map[county] = []\n",
        "        # Observed trace\n",
        "        county_trace_map[county].append(current_trace_idx)\n",
        "        current_trace_idx += 1\n",
        "        # Linear trace\n",
        "        county_trace_map[county].append(current_trace_idx)\n",
        "        current_trace_idx += 1\n",
        "        # ARIMA trace if it exists\n",
        "        df_c = forecast_df[forecast_df[\"county_name\"] == county]\n",
        "        df_c_arima_valid = df_c.dropna(subset=[\"ARIMA_Forecast\"])\n",
        "        if not df_c_arima_valid.empty:\n",
        "             county_trace_map[county].append(current_trace_idx)\n",
        "             current_trace_idx += 1 # Only increment if ARIMA trace was added\n",
        "\n",
        "\n",
        "    buttons = []\n",
        "    for i, county in enumerate(county_options):\n",
        "        # Create a visibility list for all traces, set traces for current county to True\n",
        "        vis = [False] * current_trace_idx # Initialize all to False\n",
        "        for trace_idx in county_trace_map[county]:\n",
        "             vis[trace_idx] = True\n",
        "\n",
        "        buttons.append(dict(label=county, method=\"update\", args=[{\"visible\": vis}]))\n",
        "\n",
        "\n",
        "    fig.update_layout(\n",
        "        updatemenus=[{\"buttons\": buttons, \"direction\": \"down\", \"showactive\": True}],\n",
        "        title=f\"Forest Cover Forecasts for State {geoID}\",\n",
        "        xaxis_title=\"Year\",\n",
        "        yaxis_title=\"Forest Cover (%)\"\n",
        "    )\n",
        "\n",
        "\n",
        "    return forecast_df, fig"
      ],
      "metadata": {
        "id": "NaUVQWYID1kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "# --- Fetch & clean state forest cover data (long format) ---\n",
        "def get_state_forest_dataframe(dcid: str, level: str = \"County\") -> pd.DataFrame:\n",
        "    from datacommons_client import DataCommonsClient\n",
        "    client = DataCommonsClient(api_key=\"AIzaSyCTI4Xz-UW_G2Q2RfknhcfdAnTHq5X5XuI\")\n",
        "\n",
        "    # print(f\"Fetching counties for {dcid} ...\") # Removed verbose print\n",
        "    try:\n",
        "        child_places = client.node.fetch_place_children(dcid, children_type=level)[dcid]\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching children for {dcid}: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    if not child_places:\n",
        "        # print(\"No child places found.\") # Removed verbose print\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    child_dcids = [place[\"dcid\"] for place in child_places]\n",
        "    # print(f\"Found {len(child_dcids)} counties\") # Removed verbose print\n",
        "\n",
        "    df = client.observations_dataframe(\n",
        "        variable_dcids=[\"LandCoverFraction_Forest\"],\n",
        "        date=\"all\",\n",
        "        entity_dcids=child_dcids\n",
        "    ).reset_index()\n",
        "\n",
        "    if df.empty:\n",
        "        # print(\"No forest cover data found.\") # Removed verbose print\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Pick correct value column\n",
        "    if \"LandCoverFraction_Forest\" in df.columns:\n",
        "        value_col = \"LandCoverFraction_Forest\"\n",
        "    elif \"value\" in df.columns:\n",
        "        value_col = \"value\"\n",
        "    else:\n",
        "        raise KeyError(f\"Expected LandCoverFraction_Forest or value, got {df.columns.tolist()}\")\n",
        "\n",
        "    df = df.rename(columns={\n",
        "        \"entity\": \"Fips\",\n",
        "        \"entity_name\": \"county_name\",\n",
        "        \"date\": \"year\",\n",
        "        value_col: \"Forest_Cover_Percent\"\n",
        "    })\n",
        "\n",
        "    # Clean\n",
        "    df[\"Fips\"] = df[\"Fips\"].str.replace(\"geoId/\", \"\")\n",
        "    df[\"year\"] = df[\"year\"].astype(int)\n",
        "\n",
        "    return df[[\"Fips\", \"county_name\", \"year\", \"Forest_Cover_Percent\"]]\n",
        "\n",
        "\n",
        "# --- Wrapper: build wide-format summary (optional) ---\n",
        "def build_state_merged_dataframe(dcid: str) -> pd.DataFrame:\n",
        "    df_forest = get_state_forest_dataframe(dcid)\n",
        "    if df_forest.empty:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df_pivot = df_forest.pivot_table(\n",
        "        index=[\"Fips\", \"county_name\"],\n",
        "        columns=\"year\",\n",
        "        values=\"Forest_Cover_Percent\"\n",
        "    ).reset_index()\n",
        "\n",
        "    df_pivot = df_pivot.rename(columns=lambda x: f\"Forest_Cover_{x}\" if isinstance(x, int) else x)\n",
        "    year_cols = [c for c in df_pivot.columns if c.startswith(\"Forest_Cover_\")]\n",
        "\n",
        "    df_pivot[\"Average_Forest_Cover\"] = df_pivot[year_cols].mean(axis=1)\n",
        "    df_pivot[\"Forest_Cover_Std_Dev\"] = df_pivot[year_cols].std(axis=1)\n",
        "\n",
        "    if \"Forest_Cover_2015\" in df_pivot and \"Forest_Cover_2019\" in df_pivot:\n",
        "        df_pivot[\"Forest_Cover_Change_2015_2019\"] = (\n",
        "            df_pivot[\"Forest_Cover_2019\"] - df_pivot[\"Forest_Cover_2015\"]\n",
        "        )\n",
        "    else:\n",
        "        df_pivot[\"Forest_Cover_Change_2015_2019\"] = None\n",
        "\n",
        "    def categorize_change(x):\n",
        "        if pd.isna(x):\n",
        "            return \"Unknown\"\n",
        "        elif x > 0:\n",
        "            return \"Increase\"\n",
        "        elif x < 0:\n",
        "            return \"Decrease\"\n",
        "        else:\n",
        "            return \"Stable\"\n",
        "\n",
        "    df_pivot[\"Canopy_Change_Category\"] = df_pivot[\"Forest_Cover_Change_2015_2019\"].apply(categorize_change)\n",
        "\n",
        "    # print(f\"Built df_merged_all_years for {dcid}: {df_pivot.shape[0]} counties, {df_pivot.shape[1]} columns\") # Removed verbose print\n",
        "    return df_pivot\n",
        "\n",
        "\n",
        "# --- Example usage with analyze_state ---\n",
        "dcid = \"geoId/13\"   # Georgia\n",
        "df_state_long = get_state_forest_dataframe(dcid)   # <-- feed this to analyze_state\n",
        "display(df_state_long.head())\n",
        "\n",
        "forecast_df, fig = analyze_state(df_state_long, geoID=\"13\", horizon=10)\n",
        "display(forecast_df.head())\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "iClIwUwfWBnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install plotly\n",
        "import plotly.graph_objects as go"
      ],
      "metadata": {
        "id": "xooUWQmmh55I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "import os\n",
        "\n",
        "# --- Dictionary of all US states with geoIds ---\n",
        "# Using the full list from earlier in the notebook for comprehensive data fetching\n",
        "all_us_state_geoids = {\n",
        "    \"Alabama\": \"geoId/01\", \"Alaska\": \"geoId/02\", \"Arizona\": \"geoId/04\", \"Arkansas\": \"geoId/05\",\n",
        "    \"California\": \"geoId/06\", \"Colorado\": \"geoId/08\", \"Connecticut\": \"geoId/09\", \"Delaware\": \"geoId/10\",\n",
        "    \"Florida\": \"geoId/12\", \"Georgia\": \"geoId/13\", \"Hawaii\": \"geoId/15\", \"Idaho\": \"geoId/16\",\n",
        "    \"Illinois\": \"geoId/17\", \"Indiana\": \"geoId/18\", \"Iowa\": \"geoId/19\", \"Kansas\": \"geoId/20\",\n",
        "    \"Kentucky\": \"geoId/21\", \"Louisiana\": \"geoId/22\", \"Maine\": \"geoId/23\", \"Maryland\": \"geoId/24\",\n",
        "    \"Massachusetts\": \"geoId/25\", \"Michigan\": \"geoId/26\", \"Minnesota\": \"geoId/27\", \"Mississippi\": \"geoId/28\",\n",
        "    \"Missouri\": \"geoId/29\", \"Montana\": \"geoId/30\", \"Nebraska\": \"geoId/31\", \"Nevada\": \"geoId/32\",\n",
        "    \"New Hampshire\": \"geoId/33\", \"New Jersey\": \"geoId/34\", \"New Mexico\": \"geoId/35\", \"New York\": \"geoId/36\",\n",
        "    \"North Carolina\": \"geoId/37\", \"North Dakota\": \"geoId/38\", \"Ohio\": \"geoId/39\", \"Oklahoma\": \"geoId/40\",\n",
        "    \"Oregon\": \"geoId/41\", \"Pennsylvania\": \"geoId/42\", \"Rhode Island\": \"geoId/44\", \"South Carolina\": \"geoId/45\",\n",
        "    \"South Dakota\": \"geoId/46\", \"Tennessee\": \"geoId/47\", \"Texas\": \"geoId/48\", \"Utah\": \"geoId/49\",\n",
        "    \"Vermont\": \"geoId/50\", \"Virginia\": \"geoId/51\", \"Washington\": \"geoId/53\", \"West Virginia\": \"geoId/54\",\n",
        "    \"Wisconsin\": \"geoId/55\", \"Wyoming\": \"geoId/56\"\n",
        "}\n",
        "\n",
        "\n",
        "# --- Function to fetch forest cover data for multiple states ---\n",
        "# This function is kept in case the master data needs to be recreated\n",
        "def get_forest_data_for_states(state_dcids: list, level: str = \"County\") -> pd.DataFrame:\n",
        "    from datacommons_client import DataCommonsClient\n",
        "    client = DataCommonsClient(api_key=\"AIzaSyCTI4Xz-UW_G2Q2RfknhcfdAnTHq5X5XuI\")\n",
        "\n",
        "    all_county_dcids = []\n",
        "\n",
        "    print(f\"Fetching counties for {len(state_dcids)} states...\")\n",
        "    for dcid in state_dcids:\n",
        "        try:\n",
        "            child_places = client.node.fetch_place_children(dcid, children_type=level)[dcid]\n",
        "            if child_places:\n",
        "                county_dcids = [place[\"dcid\"] for place in child_places]\n",
        "                all_county_dcids.extend(county_dcids)\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching children for {dcid}: {e}\")\n",
        "            continue\n",
        "\n",
        "    if not all_county_dcids:\n",
        "        print(\"No counties found for the specified states.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Fetch data for all counties in one call\n",
        "    print(f\"Fetching forest cover data for {len(all_county_dcids)} counties...\")\n",
        "    df = client.observations_dataframe(\n",
        "        variable_dcids=[\"LandCoverFraction_Forest\"],\n",
        "        date=\"all\", # Fetch all available years\n",
        "        entity_dcids=all_county_dcids\n",
        "    ).reset_index()\n",
        "\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"No forest cover data found for the specified counties.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Pick correct value column\n",
        "    if \"LandCoverFraction_Forest\" in df.columns:\n",
        "        value_col = \"LandCoverFraction_Forest\"\n",
        "    elif \"value\" in df.columns:\n",
        "        value_col = \"value\"\n",
        "    else:\n",
        "        raise KeyError(f\"Expected LandCoverFraction_Forest or value, got {df.columns.tolist()}\")\n",
        "\n",
        "    df = df.rename(columns={\n",
        "        \"entity\": \"Fips\",\n",
        "        \"entity_name\": \"county_name\",\n",
        "        \"date\": \"year\",\n",
        "        value_col: \"Forest_Cover_Percent\"\n",
        "    })\n",
        "\n",
        "    df[\"Fips\"] = df[\"Fips\"].str.replace(\"geoId/\", \"\")\n",
        "    df[\"year\"] = df[\"year\"].astype(int)\n",
        "    df['state_geoId'] = 'geoId/' + df['Fips'].str[:2]\n",
        "\n",
        "\n",
        "    return df[[\"state_geoId\", \"Fips\", \"county_name\", \"year\", \"Forest_Cover_Percent\"]]\n",
        "\n",
        "\n",
        "# --- Function to filter the master dataframe by state ---\n",
        "def filter_data_by_state(master_df: pd.DataFrame, state_dcid: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Filters the master forest cover dataframe to return data for a specific state.\n",
        "\n",
        "    Args:\n",
        "        master_df (pd.DataFrame): The dataframe containing data for all states.\n",
        "        state_dcid (str): The geoId of the state to filter by (e.g., \"geoId/13\").\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A dataframe containing data only for the specified state,\n",
        "                      or an empty dataframe if the state is not found or has no data.\n",
        "    \"\"\"\n",
        "    if 'state_geoId' not in master_df.columns:\n",
        "        print(\"Error: 'state_geoId' column not found in the master dataframe.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df_state = master_df[master_df['state_geoId'] == state_dcid].copy()\n",
        "    return df_state\n",
        "\n",
        "\n",
        "# --- Main process: Load master data, filter for 4 states, and save ---\n",
        "\n",
        "# Define the list of state geoIds for filtering\n",
        "state_geoIds_to_filter = [\n",
        "    \"geoId/13\", # Georgia\n",
        "    \"geoId/36\", # New York\n",
        "    \"geoId/06\", # California\n",
        "    \"geoId/48\"  # Texas\n",
        "]\n",
        "\n",
        "# Define the path for the master CSV file\n",
        "master_csv_path = os.path.join(\"data\", \"master_forest_data.csv\") # Assuming it's in the 'data' directory\n",
        "\n",
        "# Load the master forest data from the CSV file\n",
        "print(f\"Loading master forest data from {master_csv_path}...\")\n",
        "if os.path.exists(master_csv_path):\n",
        "    try:\n",
        "        master_forest_data_df = pd.read_csv(master_csv_path)\n",
        "        print(\"Master forest data loaded successfully.\")\n",
        "        display(master_forest_data_df.head())\n",
        "\n",
        "        # Create a directory to save the filtered state data files if it doesn't exist\n",
        "        filtered_data_dir = \"filtered_state_data\" # New directory for filtered data\n",
        "        os.makedirs(filtered_data_dir, exist_ok=True)\n",
        "        print(f\"\\nEnsured directory '{filtered_data_dir}' exists for saving filtered state data.\")\n",
        "\n",
        "        # --- Dictionary of all US states with geoIds (needed for naming files) ---\n",
        "        # Assuming all_us_state_geoids is defined in a previous cell\n",
        "        all_us_state_geoids = {\n",
        "            \"Alabama\": \"geoId/01\", \"Alaska\": \"geoId/02\", \"Arizona\": \"geoId/04\", \"Arkansas\": \"geoId/05\",\n",
        "            \"California\": \"geoId/06\", \"Colorado\": \"geoId/08\", \"Connecticut\": \"geoId/09\", \"Delaware\": \"geoId/10\",\n",
        "            \"Florida\": \"geoId/12\", \"Georgia\": \"geoId/13\", \"Hawaii\": \"geoId/15\", \"Idaho\": \"geoId/16\",\n",
        "            \"Illinois\": \"geoId/17\", \"Indiana\": \"geoId/18\", \"Iowa\": \"geoId/19\", \"Kansas\": \"geoId/20\",\n",
        "            \"Kentucky\": \"geoId/21\", \"Louisiana\": \"geoId/22\", \"Maine\": \"geoId/23\", \"Maryland\": \"geoId/24\",\n",
        "            \"Massachusetts\": \"geoId/25\", \"Michigan\": \"geoId/26\", \"Minnesota\": \"geoId/27\", \"Mississippi\": \"geoId/28\",\n",
        "            \"Missouri\": \"geoId/29\", \"Montana\": \"geoId/30\", \"Nebraska\": \"geoId/31\", \"Nevada\": \"geoId/32\",\n",
        "            \"New Hampshire\": \"geoId/33\", \"New Jersey\": \"geoId/34\", \"New Mexico\": \"geoId/35\", \"New York\": \"geoId/36\",\n",
        "            \"North Carolina\": \"geoId/37\", \"North Dakota\": \"geoId/38\", \"Ohio\": \"geoId/39\", \"Oklahoma\": \"geoId/40\",\n",
        "            \"Oregon\": \"geoId/41\", \"Pennsylvania\": \"geoId/42\", \"Rhode Island\": \"geoId/44\", \"South Carolina\": \"geoId/45\",\n",
        "            \"South Dakota\": \"geoId/46\", \"Tennessee\": \"geoId/47\", \"Texas\": \"geoId/48\", \"Utah\": \"geoId/49\",\n",
        "            \"Vermont\": \"geoId/50\", \"Virginia\": \"geoId/51\", \"Washington\": \"geoId/53\", \"West Virginia\": \"geoId/54\",\n",
        "            \"Wisconsin\": \"geoId/55\", \"Wyoming\": \"geoId/56\"\n",
        "        }\n",
        "\n",
        "        # Reverse mapping for file naming\n",
        "        state_name_mapping = {v: k for k, v in all_us_state_geoids.items()}\n",
        "\n",
        "\n",
        "        # Loop through the specified state geoIds and filter the master dataframe\n",
        "        state_dataframes = {}\n",
        "        print(\"\\nFiltering master data and saving results for selected states...\")\n",
        "        for state_dcid in state_geoIds_to_filter:\n",
        "            state_name = state_name_mapping.get(state_dcid, state_dcid.replace(\"geoId/\", \"Unknown_State_\"))\n",
        "            state_name_lower_underscore = state_name.lower().replace(' ', '_')\n",
        "\n",
        "            print(f\"\\nFiltering data for state: {state_name} ({state_dcid})\")\n",
        "\n",
        "            # Use the filter_data_by_state function\n",
        "            df_state = filter_data_by_state(master_forest_data_df, state_dcid)\n",
        "\n",
        "            if not df_state.empty:\n",
        "                state_dataframes[state_dcid] = df_state\n",
        "                print(f\"Created dataframe for {state_dcid} with {len(df_state['Fips'].unique())} counties.\")\n",
        "                display(df_state.head())\n",
        "\n",
        "                # Save the filtered state dataframe to a file\n",
        "                file_name = f\"{state_name_lower_underscore}_forest_data.csv\"\n",
        "                file_path = os.path.join(filtered_data_dir, file_name)\n",
        "\n",
        "                df_state.to_csv(file_path, index=False)\n",
        "                print(f\"Filtered data saved to: {file_path}\")\n",
        "\n",
        "            else:\n",
        "                print(f\"\\nNo data found for state {state_dcid} in the master dataframe.\")\n",
        "\n",
        "        print(\"\\nFinished filtering and saving data for selected states.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading master data from CSV: {e}\")\n",
        "        master_forest_data_df = pd.DataFrame() # Ensure master_forest_data_df is empty on error\n",
        "else:\n",
        "    print(f\"Master forest data CSV not found at {master_csv_path}. Please run the cell to create it first.\")\n",
        "    master_forest_data_df = pd.DataFrame() # Ensure master_forest_data_df is empty if file not found\n",
        "\n",
        "\n",
        "# After running this cell, you will have CSV files like\n",
        "# filtered_state_data/georgia_forest_data.csv\n",
        "# filtered_state_data/new_york_forest_data.csv\n",
        "# etc.\n",
        "# These files contain the raw historical data filtered by state.\n",
        "# The Streamlit app will need to be updated to load these files and then run analyze_state on them."
      ],
      "metadata": {
        "id": "Q3kz1Y6nN49N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Add the directory containing analyze_state to the path if needed\n",
        "# Assuming analyze_state is defined in a previous cell in this notebook.\n",
        "\n",
        "# Define the path for the master CSV file (assuming it was saved previously)\n",
        "master_csv_path = os.path.join(\"data\", \"master_forest_data.csv\")\n",
        "\n",
        "# Define the expected column names and their data types\n",
        "expected_dtypes = {\n",
        "    'state_geoId': str,\n",
        "    'Fips': str,\n",
        "    'county_name': str,\n",
        "    'year': int,\n",
        "    'Forest_Cover_Percent': float\n",
        "}\n",
        "\n",
        "# Load the master forest data from the CSV file\n",
        "print(f\"Loading master forest data from {master_csv_path} with specified dtypes...\")\n",
        "master_forest_data_df = pd.DataFrame() # Initialize as empty\n",
        "\n",
        "if os.path.exists(master_csv_path):\n",
        "    try:\n",
        "        # Read the CSV with specified column names and dtypes\n",
        "        master_forest_data_df = pd.read_csv(master_csv_path, dtype=expected_dtypes)\n",
        "        print(\"Master forest data loaded successfully with specified dtypes.\")\n",
        "        # Ensure consistent column names and types if necessary (already handled by dtype)\n",
        "        master_forest_data_df.columns = [c.lower() for c in master_forest_data_df.columns]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading master data from CSV: {e}\")\n",
        "\n",
        "else:\n",
        "    print(f\"Master forest data CSV not found at {master_csv_path}. Please run the cell to create it first.\")\n",
        "\n",
        "master_forest_data_df.head(10)\n",
        "# Proceed only if the master dataframe was loaded successfully\n",
        "if not master_forest_data_df.empty:\n",
        "\n",
        "    # Identify all unique state_geoIds in the master dataframe\n",
        "    all_unique_state_dcids = master_forest_data_df['state_geoid'].unique().tolist()\n",
        "    print(f\"\\nFound {len(all_unique_state_dcids)} unique states in the master dataframe: {all_unique_state_dcids}\")\n",
        "\n",
        "\n",
        "    # Define the forecast horizon\n",
        "    forecast_horizon = 10 # You can adjust this\n",
        "\n",
        "    # Create a directory to save the forecast data files if it doesn't exist\n",
        "    forecast_data_dir = \"forecast_data\"\n",
        "    os.makedirs(forecast_data_dir, exist_ok=True)\n",
        "    print(f\"\\nEnsured directory '{forecast_data_dir}' exists for saving forecast data.\")\n",
        "\n",
        "    # --- Dictionary of all US states with geoIds (needed for naming files) ---\n",
        "    # Assuming all_us_state_geoids is defined in a previous cell\n",
        "    all_us_state_geoids = {\n",
        "        \"Alabama\": \"geoId/01\", \"Alaska\": \"geoId/02\", \"Arizona\": \"geoId/04\", \"Arkansas\": \"geoId/05\",\n",
        "        \"California\": \"geoId/06\", \"Colorado\": \"geoId/08\", \"Connecticut\": \"geoId/09\", \"Delaware\": \"geoId/10\",\n",
        "        \"Florida\": \"geoId/12\", \"Georgia\": \"geoId/13\", \"Hawaii\": \"geoId/15\", \"Idaho\": \"geoId/16\",\n",
        "        \"Illinois\": \"geoId/17\", \"Indiana\": \"geoId/18\", \"Iowa\": \"Iowa\", \"Kansas\": \"geoId/20\",\n",
        "        \"Kentucky\": \"geoId/21\", \"Louisiana\": \"geoId/22\", \"Maine\": \"geoId/23\", \"Maryland\": \"geoId/24\",\n",
        "        \"Massachusetts\": \"geoId/25\", \"Michigan\": \"geoId/26\", \"Minnesota\": \"geoId/27\", \"Mississippi\": \"geoId/28\",\n",
        "        \"Missouri\": \"geoId/29\", \"Montana\": \"geoId/30\", \"Nebraska\": \"geoId/31\", \"Nevada\": \"geoId/32\",\n",
        "        \"New Hampshire\": \"geoId/33\", \"New Jersey\": \"geoId/34\", \"New Mexico\": \"geoId/35\", \"New York\": \"geoId/36\",\n",
        "        \"North Carolina\": \"geoId/37\", \"North Dakota\": \"geoId/38\", \"Ohio\": \"geoId/39\", \"Oklahoma\": \"geoId/40\",\n",
        "        \"Oregon\": \"geoId/41\", \"Pennsylvania\": \"geoId/42\", \"Rhode Island\": \"geoId/44\", \"South Carolina\": \"geoId/45\",\n",
        "        \"South Dakota\": \"geoId/46\", \"Tennessee\": \"geoId/47\", \"Texas\": \"geoId/48\", \"Utah\": \"geoId/49\",\n",
        "        \"Vermont\": \"geoId/50\", \"Virginia\": \"geoId/51\", \"Washington\": \"geoId/53\", \"West Virginia\": \"geoId/54\",\n",
        "        \"Wisconsin\": \"geoId/55\", \"Wyoming\": \"geoId/56\"\n",
        "    }\n",
        "\n",
        "    # Reverse mapping for file naming\n",
        "    state_name_mapping = {v.replace(\"geoId/\", \"\"): k.lower().replace(' ', '_') for k, v in all_us_state_geoids.items()}\n",
        "\n",
        "\n",
        "    # --- Loop through ALL unique states found, filter master data, analyze, and save results ---\n",
        "    print(\"\\nStarting analysis and saving results for all unique states found in master data...\")\n",
        "    # Use all_unique_state_dcids for the loop\n",
        "    for state_dcid_full in all_unique_state_dcids:\n",
        "        # Extract the numeric state ID from the full geoId (e.g., \"geoId/13\" -> \"13\")\n",
        "        state_id = state_dcid_full.split(\"/\")[1]\n",
        "        state_name_lower_underscore = state_name_mapping.get(state_id, f\"unknown_state_{state_id}\")\n",
        "\n",
        "\n",
        "        print(f\"\\nProcessing state ID: {state_id} (Mapped name: {state_name_lower_underscore})\")\n",
        "\n",
        "        # 1. Filter the master data for the current state\n",
        "        # Filter based on the full state_geoId column in the master dataframe\n",
        "        df_state_long = master_forest_data_df[master_forest_data_df['state_geoid'] == state_dcid_full].copy()\n",
        "\n",
        "\n",
        "        if df_state_long.empty:\n",
        "            print(f\"No historical data found for {state_dcid_full}. Skipping analysis and saving.\")\n",
        "            continue\n",
        "\n",
        "        # 2. Run the analysis (Linear Regression and ARIMA)\n",
        "        # Assuming analyze_state is defined in a previous cell and returns forecast_df and fig\n",
        "        # We only need forecast_df here. The analyze_state function expects 'fips', 'county_name', 'year', 'forest_cover_percent'\n",
        "        # columns and they should be lower case based on its internal logic.\n",
        "        # Ensure df_state_long has these columns in lower case before passing.\n",
        "        df_state_long.columns = [c.lower() for c in df_state_long.columns]\n",
        "\n",
        "        # analyze_state expects geoID to be the numeric part (e.g., \"13\")\n",
        "        analysis_result = analyze_state(df_state_long, geoID=state_id, horizon=forecast_horizon)\n",
        "\n",
        "        # Check if analyze_state returned valid results before unpacking\n",
        "        if analysis_result is not None:\n",
        "            forecast_df, _ = analysis_result # Unpack the results\n",
        "            if not forecast_df.empty:\n",
        "                # 3. Save the forecast dataframe to a file\n",
        "                file_name = f\"{state_name_lower_underscore}_forecast_data.csv\"\n",
        "                file_path = os.path.join(forecast_data_dir, file_name)\n",
        "\n",
        "                forecast_df.to_csv(file_path, index=False)\n",
        "                print(f\"Analysis results saved to: {file_path}\")\n",
        "            else:\n",
        "                print(f\"Analysis for {state_dcid_full} did not produce a valid forecast dataframe. Skipping save.\")\n",
        "        else:\n",
        "             print(f\"Analysis for {state_dcid_full} returned None. Skipping save.\")\n",
        "\n",
        "\n",
        "    print(\"\\nFinished processing and saving results for all unique states found in master data.\")\n",
        "\n",
        "    # After running this cell, you will have CSV files like\n",
        "    # forecast_data/georgia_forecast_data.csv\n",
        "    # forecast_data/new_york_forecast_data.csv\n",
        "    # etc.\n",
        "    # These files will be used by the Streamlit app.\n",
        "else:\n",
        "    print(\"\\nMaster forest data DataFrame is empty. Cannot proceed with state filtering and analysis.\")"
      ],
      "metadata": {
        "id": "gpcwB6oJwFGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import plotly.io as pio\n",
        "\n",
        "# --- Dictionary of states with geoIds ---\n",
        "state_geoids = {\n",
        "    \"Alabama\": \"geoId/01\",\n",
        "    \"Alaska\": \"geoId/02\",\n",
        "    \"Arizona\": \"geoId/04\",\n",
        "    \"Arkansas\": \"geoId/05\",\n",
        "    \"California\": \"geoId/06\",\n",
        "    \"Colorado\": \"geoId/08\",\n",
        "    \"Connecticut\": \"geoId/09\",\n",
        "    \"Delaware\": \"geoId/10\",\n",
        "    \"Florida\": \"geoId/12\",\n",
        "    \"Georgia\": \"geoId/13\",\n",
        "    \"Hawaii\": \"geoId/15\",\n",
        "    \"Idaho\": \"geoId/16\",\n",
        "    \"Illinois\": \"geoId/17\",\n",
        "    \"Indiana\": \"geoId/18\",\n",
        "    \"Iowa\": \"geoId/19\",\n",
        "    \"Kansas\": \"geoId/20\",\n",
        "    \"Kentucky\": \"geoId/21\",\n",
        "    \"Louisiana\": \"geoId/22\",\n",
        "    \"Maine\": \"geoId/23\",\n",
        "    \"Maryland\": \"geoId/24\",\n",
        "    \"Massachusetts\": \"geoId/25\",\n",
        "    \"Michigan\": \"geoId/26\",\n",
        "    \"Minnesota\": \"geoId/27\",\n",
        "    \"Mississippi\": \"geoId/28\",\n",
        "    \"Missouri\": \"geoId/29\",\n",
        "    \"Montana\": \"geoId/30\",\n",
        "    \"Nebraska\": \"geoId/31\",\n",
        "    \"Nevada\": \"geoId/32\",\n",
        "    \"New Hampshire\": \"geoId/33\",\n",
        "    \"New Jersey\": \"geoId/34\",\n",
        "    \"New Mexico\": \"geoId/35\",\n",
        "    \"New York\": \"geoId/36\",\n",
        "    \"North Carolina\": \"geoId/37\",\n",
        "    \"North Dakota\": \"geoId/38\",\n",
        "    \"Ohio\": \"geoId/39\",\n",
        "    \"Oklahoma\": \"geoId/40\",\n",
        "    \"Oregon\": \"geoId/41\",\n",
        "    \"Pennsylvania\": \"geoId/42\",\n",
        "    \"Rhode Island\": \"geoId/44\",\n",
        "    \"South Carolina\": \"geoId/45\",\n",
        "    \"South Dakota\": \"geoId/46\",\n",
        "    \"Tennessee\": \"geoId/47\",\n",
        "    \"Texas\": \"geoId/48\",\n",
        "    \"Utah\": \"geoId/49\",\n",
        "    \"Vermont\": \"geoId/50\",\n",
        "    \"Virginia\": \"geoId/51\",\n",
        "    \"Washington\": \"geoId/53\",\n",
        "    \"West Virginia\": \"geoId/54\",\n",
        "    \"Wisconsin\": \"geoId/55\",\n",
        "    \"Wyoming\": \"geoId/56\"\n",
        "}\n",
        "\n",
        "# --- Dropdown widget for states ---\n",
        "state_dropdown = widgets.Dropdown(\n",
        "    options=state_geoids,\n",
        "    value=\"geoId/13\",   # default = Georgia\n",
        "    description=\"State:\"\n",
        ")\n",
        "\n",
        "# --- Horizon input (how many years to forecast) ---\n",
        "horizon_slider = widgets.IntSlider(\n",
        "    value=10, min=5, max=30, step=1,\n",
        "    description=\"Horizon (yrs):\"\n",
        ")\n",
        "\n",
        "# --- Button to run analysis ---\n",
        "run_button = widgets.Button(\n",
        "    description=\"Analyze State\",\n",
        "    button_style=\"success\"\n",
        ")\n",
        "\n",
        "# --- Output area ---\n",
        "output = widgets.Output()\n",
        "\n",
        "# --- Handler for button click ---\n",
        "def on_run_clicked(b):\n",
        "    with output:\n",
        "        clear_output()\n",
        "        dcid = state_dropdown.value\n",
        "        geoID = dcid.split(\"/\")[1]  # extract numeric ID (e.g. \"13\")\n",
        "        horizon = horizon_slider.value\n",
        "\n",
        "        df_state_long = get_state_forest_dataframe(dcid)\n",
        "\n",
        "        if df_state_long.empty:\n",
        "            print(\"No data found for this state.\")\n",
        "            return\n",
        "\n",
        "        forecast_df, fig = analyze_state(df_state_long, geoID=geoID, horizon=horizon)\n",
        "        display(forecast_df.head())   # show forecast table\n",
        "\n",
        "        # Use plotly.io.show with a renderer compatible with widgets\n",
        "        pio.show(fig, renderer=\"notebook_connected\")\n",
        "# Bind button\n",
        "run_button.on_click(on_run_clicked)\n",
        "\n",
        "# --- Display widgets ---\n",
        "display(state_dropdown, horizon_slider, run_button, output)\n"
      ],
      "metadata": {
        "id": "9EV6bJXrf6eL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit\n",
        "!pip install pyngrok\n",
        "!pip install plotly"
      ],
      "metadata": {
        "id": "sZtzBW7en-Po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dash[notebook] plotly"
      ],
      "metadata": {
        "id": "_uCim7xRydt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "from dash import Dash, dcc, html, Input, Output, dash_table\n",
        "\n",
        "# --- Dictionary mapping state geoIds to full names ---\n",
        "all_us_state_geoids = {\n",
        "    \"Alabama\": \"geoId/01\", \"Alaska\": \"geoId/02\", \"Arizona\": \"geoId/04\", \"Arkansas\": \"geoId/05\",\n",
        "    \"California\": \"geoId/06\", \"Colorado\": \"geoId/08\", \"Connecticut\": \"geoId/09\", \"Delaware\": \"geoId/10\",\n",
        "    \"Florida\": \"geoId/12\", \"Georgia\": \"geoId/13\", \"Hawaii\": \"geoId/15\", \"Idaho\": \"geoId/16\",\n",
        "    \"Illinois\": \"geoId/17\", \"Indiana\": \"geoId/18\", \"Iowa\": \"geoId/19\", \"Kansas\": \"geoId/20\",\n",
        "    \"Kentucky\": \"geoId/21\", \"Louisiana\": \"geoId/22\", \"Maine\": \"geoId/23\", \"Maryland\": \"geoId/24\",\n",
        "    \"Massachusetts\": \"geoId/25\", \"Michigan\": \"geoId/26\", \"Minnesota\": \"geoId/27\", \"Mississippi\": \"geoId/28\",\n",
        "    \"Missouri\": \"geoId/29\", \"Montana\": \"geoId/30\", \"Nebraska\": \"geoId/31\", \"Nevada\": \"geoId/32\",\n",
        "    \"New Hampshire\": \"geoId/33\", \"New Jersey\": \"geoId/34\", \"New Mexico\": \"geoId/35\", \"New York\": \"geoId/36\",\n",
        "    \"North Carolina\": \"geoId/37\", \"North Dakota\": \"geoId/38\", \"Ohio\": \"geoId/39\", \"Oklahoma\": \"geoId/40\",\n",
        "    \"Oregon\": \"geoId/41\", \"Pennsylvania\": \"geoId/42\", \"Rhode Island\": \"geoId/44\", \"South Carolina\": \"geoId/45\",\n",
        "    \"South Dakota\": \"geoId/46\", \"Tennessee\": \"geoId/47\", \"Texas\": \"geoId/48\", \"Utah\": \"geoId/49\",\n",
        "    \"Vermont\": \"geoId/50\", \"Virginia\": \"geoId/51\", \"Washington\": \"geoId/53\", \"West Virginia\": \"geoId/54\",\n",
        "    \"Wisconsin\": \"geoId/55\", \"Wyoming\": \"geoId/56\"\n",
        "}\n",
        "\n",
        "# --- Forecast data directory ---\n",
        "forecast_data_dir = \"forecast_data\"\n",
        "state_forecast_files = {}\n",
        "\n",
        "if os.path.exists(forecast_data_dir):\n",
        "    for filename in os.listdir(forecast_data_dir):\n",
        "        if filename.endswith(\"_forecast_data.csv\"):\n",
        "            state_name_lower_underscore = filename.replace(\"_forecast_data.csv\", \"\")\n",
        "            state_name = state_name_lower_underscore.replace('_', ' ').title()\n",
        "            matched_state_name = None\n",
        "            for known_name in all_us_state_geoids.keys():\n",
        "                if known_name.lower().replace(' ', '_') == state_name_lower_underscore:\n",
        "                    matched_state_name = known_name\n",
        "                    break\n",
        "            if matched_state_name:\n",
        "                state_forecast_files[matched_state_name] = os.path.join(forecast_data_dir, filename)\n",
        "            else:\n",
        "                state_forecast_files[state_name] = os.path.join(forecast_data_dir, filename)\n",
        "\n",
        "# --- Dash App ---\n",
        "app = Dash(__name__)\n",
        "\n",
        "app.layout = html.Div([\n",
        "    html.H1(\"Forest Cover Trends by County\", style={\"color\": \"white\"}),\n",
        "    html.Label(\"Select a State:\", style={\"color\": \"white\"}),\n",
        "    dcc.Dropdown(\n",
        "        id=\"state-dropdown\",\n",
        "        options=[{\"label\": k, \"value\": k} for k in sorted(state_forecast_files.keys())],\n",
        "        value=list(state_forecast_files.keys())[0] if state_forecast_files else None\n",
        "    ),\n",
        "    html.Br(),\n",
        "    html.Label(\"Select a County:\", style={\"color\": \"white\"}),\n",
        "    dcc.Dropdown(id=\"county-dropdown\"),\n",
        "    html.Br(),\n",
        "    dcc.Graph(id=\"forecast-plot\"),\n",
        "    html.Div([\n",
        "        html.H3(\"Forecast Data Table\", style={'color': 'white'}),\n",
        "        dash_table.DataTable(\n",
        "            id='forecast-table',\n",
        "            columns=[],  # will be filled dynamically\n",
        "            data=[],\n",
        "            style_table={'overflowX': 'auto'},\n",
        "            style_header={\n",
        "                'backgroundColor': '#1e1e1e',\n",
        "                'color': 'white',\n",
        "                'fontWeight': 'bold'\n",
        "            },\n",
        "            style_cell={\n",
        "                'backgroundColor': '#2d2d2d',\n",
        "                'color': 'white',\n",
        "                'textAlign': 'left',\n",
        "                'padding': '8px',\n",
        "            },\n",
        "            style_data_conditional=[\n",
        "                {\n",
        "                    'if': {'row_index': 'odd'},\n",
        "                    'backgroundColor': '#3a3a3a'\n",
        "                }\n",
        "            ],\n",
        "            page_size=10,\n",
        "        )\n",
        "    ])\n",
        "], style={\"backgroundColor\": \"#111111\", \"padding\": \"20px\"})\n",
        "\n",
        "# --- Callbacks ---\n",
        "@app.callback(\n",
        "    Output(\"county-dropdown\", \"options\"),\n",
        "    Output(\"county-dropdown\", \"value\"),\n",
        "    Input(\"state-dropdown\", \"value\")\n",
        ")\n",
        "def update_county_dropdown(state_name):\n",
        "    if not state_name:\n",
        "        return [], None\n",
        "    file_path = state_forecast_files[state_name]\n",
        "    df = pd.read_csv(file_path)\n",
        "    counties = sorted(df[\"county_name\"].unique())\n",
        "    return [{\"label\": c, \"value\": c} for c in counties], counties[0] if counties else None\n",
        "\n",
        "@app.callback(\n",
        "    Output(\"forecast-plot\", \"figure\"),\n",
        "    Output(\"forecast-table\", \"columns\"),\n",
        "    Output(\"forecast-table\", \"data\"),\n",
        "    Input(\"state-dropdown\", \"value\"),\n",
        "    Input(\"county-dropdown\", \"value\")\n",
        ")\n",
        "def update_plot(state_name, county_name):\n",
        "    if not state_name or not county_name:\n",
        "        return go.Figure(), [], []\n",
        "\n",
        "    file_path = state_forecast_files[state_name]\n",
        "    df = pd.read_csv(file_path)\n",
        "    df[\"year\"] = df[\"year\"].astype(int)\n",
        "    df[\"Linear_Forecast\"] = df[\"Linear_Forecast\"].astype(float)\n",
        "    df[\"ARIMA_Forecast\"] = pd.to_numeric(df[\"ARIMA_Forecast\"], errors=\"coerce\")\n",
        "\n",
        "    county_data = df[df[\"county_name\"] == county_name]\n",
        "\n",
        "    fig = go.Figure()\n",
        "    last_historical_year = 2019\n",
        "    county_observed = county_data[county_data[\"year\"] <= last_historical_year]\n",
        "\n",
        "    if not county_observed.empty:\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=county_observed[\"year\"], y=county_observed[\"Linear_Forecast\"],\n",
        "            mode=\"markers+lines\", name=\"Observed\", marker=dict(color=\"black\")\n",
        "        ))\n",
        "\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=county_data[\"year\"], y=county_data[\"Linear_Forecast\"],\n",
        "        mode=\"lines\", name=\"Linear Forecast\", line=dict(dash=\"dash\", color=\"red\")\n",
        "    ))\n",
        "\n",
        "    county_arima_valid = county_data.dropna(subset=[\"ARIMA_Forecast\"])\n",
        "    if not county_arima_valid.empty:\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=county_arima_valid[\"year\"], y=county_arima_valid[\"ARIMA_Forecast\"],\n",
        "            mode=\"lines\", name=\"ARIMA Forecast\", line=dict(color=\"blue\")\n",
        "        ))\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=f\"Forest Cover Forecast for {county_name}, {state_name}\",\n",
        "        xaxis_title=\"Year\",\n",
        "        yaxis_title=\"Forest Cover (%)\",\n",
        "        hovermode=\"x unified\",\n",
        "        plot_bgcolor=\"#111111\",\n",
        "        paper_bgcolor=\"#111111\",\n",
        "        font=dict(color=\"white\")\n",
        "    )\n",
        "\n",
        "    # Update table\n",
        "    filtered_df = df[df[\"county_name\"] == county_name]\n",
        "    columns = [{\"name\": i, \"id\": i} for i in filtered_df.columns]\n",
        "    data = filtered_df.to_dict(\"records\")\n",
        "\n",
        "    return fig, columns, data\n",
        "\n",
        "# --- Run inside Colab/Jupyter ---\n",
        "app.run(mode=\"inline\")\n"
      ],
      "metadata": {
        "id": "XJ6fdeKh2d3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from dash import Dash, dcc, html, Input, Output, dash_table\n",
        "\n",
        "# --- Dictionary mapping state geoIds to full names ---\n",
        "all_us_state_geoids = {\n",
        "    \"Alabama\": \"geoId/01\", \"Alaska\": \"geoId/02\", \"Arizona\": \"geoId/04\", \"Arkansas\": \"geoId/05\",\n",
        "    \"California\": \"geoId/06\", \"Colorado\": \"geoId/08\", \"Connecticut\": \"geoId/09\", \"Delaware\": \"geoId/10\",\n",
        "    \"Florida\": \"geoId/12\", \"Georgia\": \"geoId/13\", \"Hawaii\": \"geoId/15\", \"Idaho\": \"geoId/16\",\n",
        "    \"Illinois\": \"geoId/17\", \"Indiana\": \"geoId/18\", \"Iowa\": \"geoId/19\", \"Kansas\": \"geoId/20\",\n",
        "    \"Kentucky\": \"geoId/21\", \"Louisiana\": \"geoId/22\", \"Maine\": \"geoId/23\", \"Maryland\": \"geoId/24\",\n",
        "    \"Massachusetts\": \"geoId/25\", \"Michigan\": \"geoId/26\", \"Minnesota\": \"geoId/27\", \"Mississippi\": \"geoId/28\",\n",
        "    \"Missouri\": \"geoId/29\", \"Montana\": \"geoId/30\", \"Nebraska\": \"geoId/31\", \"Nevada\": \"geoId/32\",\n",
        "    \"New Hampshire\": \"geoId/33\", \"New Jersey\": \"geoId/34\", \"New Mexico\": \"geoId/35\", \"New York\": \"geoId/36\",\n",
        "    \"North Carolina\": \"geoId/37\", \"North Dakota\": \"geoId/38\", \"Ohio\": \"geoId/39\", \"Oklahoma\": \"geoId/40\",\n",
        "    \"Oregon\": \"geoId/41\", \"Pennsylvania\": \"geoId/42\", \"Rhode Island\": \"geoId/44\", \"South Carolina\": \"geoId/45\",\n",
        "    \"South Dakota\": \"geoId/46\", \"Tennessee\": \"geoId/47\", \"Texas\": \"geoId/48\", \"Utah\": \"geoId/49\",\n",
        "    \"Vermont\": \"geoId/50\", \"Virginia\": \"geoId/51\", \"Washington\": \"geoId/53\", \"West Virginia\": \"geoId/54\",\n",
        "    \"Wisconsin\": \"geoId/55\", \"Wyoming\": \"geoId/56\"\n",
        "}\n",
        "\n",
        "# --- Forecast data directory ---\n",
        "forecast_data_dir = \"forecast_data\"\n",
        "state_forecast_files = {}\n",
        "\n",
        "if os.path.exists(forecast_data_dir):\n",
        "    for filename in os.listdir(forecast_data_dir):\n",
        "        if filename.endswith(\"_forecast_data.csv\"):\n",
        "            state_name_lower_underscore = filename.replace(\"_forecast_data.csv\", \"\")\n",
        "            state_name = state_name_lower_underscore.replace('_', ' ').title()\n",
        "            matched_state_name = None\n",
        "            for known_name in all_us_state_geoids.keys():\n",
        "                if known_name.lower().replace(' ', '_') == state_name_lower_underscore:\n",
        "                    matched_state_name = known_name\n",
        "                    break\n",
        "            if matched_state_name:\n",
        "                state_forecast_files[matched_state_name] = os.path.join(forecast_data_dir, filename)\n",
        "            else:\n",
        "                state_forecast_files[state_name] = os.path.join(forecast_data_dir, filename)\n",
        "\n",
        "# --- Dash App ---\n",
        "app = Dash(__name__)\n",
        "\n",
        "app.layout = html.Div([\n",
        "    html.H1(\"Forest Cover Trends by County\", style={'color': 'white'}),\n",
        "\n",
        "    # --- State Selector ---\n",
        "    html.Label(\"Select a State:\", style={'color': 'white'}),\n",
        "    dcc.Dropdown(\n",
        "        id=\"state-dropdown\",\n",
        "        options=[{\"label\": k, \"value\": k} for k in sorted(state_forecast_files.keys())],\n",
        "        value=list(state_forecast_files.keys())[0] if state_forecast_files else None,\n",
        "        style={'color': 'black'}\n",
        "    ),\n",
        "\n",
        "    html.Br(),\n",
        "\n",
        "    # --- County Selector ---\n",
        "    html.Label(\"Select a County:\", style={'color': 'white'}),\n",
        "    dcc.Dropdown(id=\"county-dropdown\", style={'color': 'black'}),\n",
        "\n",
        "    html.Br(),\n",
        "\n",
        "    # --- Threshold Type Selector ---\n",
        "    html.Label(\"Select Threshold Method:\", style={'color': 'white'}),\n",
        "    dcc.RadioItems(\n",
        "        id=\"threshold-method\",\n",
        "        options=[\n",
        "            {\"label\": \"Quantile-based\", \"value\": \"quantile\"},\n",
        "            {\"label\": \"Fixed thresholds (-4%, +1%)\", \"value\": \"fixed\"}\n",
        "        ],\n",
        "        value=\"quantile\",\n",
        "        labelStyle={'display': 'inline-block', 'margin-right': '20px', 'color': 'white'}\n",
        "    ),\n",
        "\n",
        "    html.Br(),\n",
        "\n",
        "    # --- Forecast Plot ---\n",
        "    dcc.Graph(id=\"forecast-plot\", style={'height': '500px'}),\n",
        "\n",
        "    # --- Forecast Data Table ---\n",
        "    html.Div([\n",
        "        html.H3(\"Forecast Data Table\", style={'color': 'white'}),\n",
        "        dash_table.DataTable(\n",
        "            id=\"forecast-table\",\n",
        "            style_table={'overflowX': 'auto'},\n",
        "            style_header={\n",
        "                'backgroundColor': '#1e1e1e', 'color': 'white', 'fontWeight': 'bold'\n",
        "            },\n",
        "            style_cell={\n",
        "                'backgroundColor': '#2d2d2d', 'color': 'white',\n",
        "                'textAlign': 'left', 'padding': '8px'\n",
        "            },\n",
        "            style_data_conditional=[\n",
        "                {'if': {'row_index': 'odd'}, 'backgroundColor': '#3a3a3a'}\n",
        "            ],\n",
        "            page_size=10,\n",
        "        )\n",
        "    ]),\n",
        "\n",
        "    html.Br(),\n",
        "\n",
        "    # --- EDA Clustering ---\n",
        "    html.H3(\"EDA Clustering by Canopy Change\", style={'color': 'white'}),\n",
        "    dcc.Graph(id=\"eda-cluster-plot\", style={'height': '700px'}),\n",
        "\n",
        "    html.Br(),\n",
        "\n",
        "    # --- Countywise Change ---\n",
        "    html.H3(\"Countywise Forest Cover Change (2015–2019)\", style={'color': 'white'}),\n",
        "    dcc.Graph(id=\"countywise-change-plot\", style={'height': '700px'}),\n",
        "])\n",
        "\n",
        "# --- Callbacks ---\n",
        "@app.callback(\n",
        "    Output(\"county-dropdown\", \"options\"),\n",
        "    Output(\"county-dropdown\", \"value\"),\n",
        "    Input(\"state-dropdown\", \"value\")\n",
        ")\n",
        "def update_county_dropdown(state_name):\n",
        "    if not state_name:\n",
        "        return [], None\n",
        "    file_path = state_forecast_files[state_name]\n",
        "    df = pd.read_csv(file_path)\n",
        "    counties = sorted(df[\"county_name\"].unique())\n",
        "    return [{\"label\": c, \"value\": c} for c in counties], counties[0] if counties else None\n",
        "\n",
        "@app.callback(\n",
        "    Output(\"forecast-plot\", \"figure\"),\n",
        "    Output(\"forecast-table\", \"columns\"),\n",
        "    Output(\"forecast-table\", \"data\"),\n",
        "    Output(\"eda-cluster-plot\", \"figure\"),\n",
        "    Output(\"countywise-change-plot\", \"figure\"),\n",
        "    Input(\"state-dropdown\", \"value\"),\n",
        "    Input(\"county-dropdown\", \"value\")\n",
        ")\n",
        "def update_all(state_name, county_name):\n",
        "    if not state_name or not county_name:\n",
        "        return go.Figure(), [], [], go.Figure(), go.Figure()\n",
        "\n",
        "    file_path = state_forecast_files[state_name]\n",
        "    df = pd.read_csv(file_path)\n",
        "    df[\"year\"] = df[\"year\"].astype(int)\n",
        "    df[\"Linear_Forecast\"] = df[\"Linear_Forecast\"].astype(float)\n",
        "    df[\"ARIMA_Forecast\"] = pd.to_numeric(df[\"ARIMA_Forecast\"], errors=\"coerce\")\n",
        "\n",
        "    # --- Forecast Plot ---\n",
        "    county_data = df[df[\"county_name\"] == county_name]\n",
        "    fig_forecast = go.Figure()\n",
        "    last_historical_year = 2019\n",
        "    county_observed = county_data[county_data[\"year\"] <= last_historical_year]\n",
        "\n",
        "    if not county_observed.empty:\n",
        "        fig_forecast.add_trace(go.Scatter(\n",
        "            x=county_observed[\"year\"], y=county_observed[\"Linear_Forecast\"],\n",
        "            mode=\"markers+lines\", name=\"Observed\", marker=dict(color=\"black\")\n",
        "        ))\n",
        "\n",
        "    fig_forecast.add_trace(go.Scatter(\n",
        "        x=county_data[\"year\"], y=county_data[\"Linear_Forecast\"],\n",
        "        mode=\"lines\", name=\"Linear Forecast\", line=dict(dash=\"dash\", color=\"red\")\n",
        "    ))\n",
        "\n",
        "    county_arima_valid = county_data.dropna(subset=[\"ARIMA_Forecast\"])\n",
        "    if not county_arima_valid.empty:\n",
        "        fig_forecast.add_trace(go.Scatter(\n",
        "            x=county_arima_valid[\"year\"], y=county_arima_valid[\"ARIMA_Forecast\"],\n",
        "            mode=\"lines\", name=\"ARIMA Forecast\", line=dict(color=\"blue\")\n",
        "        ))\n",
        "\n",
        "    fig_forecast.update_layout(\n",
        "        title=f\"Forest Cover Forecast for {county_name}, {state_name}\",\n",
        "        xaxis_title=\"Year\", yaxis_title=\"Forest Cover (%)\",\n",
        "        hovermode=\"x unified\",\n",
        "        plot_bgcolor=\"#111111\", paper_bgcolor=\"#111111\", font=dict(color=\"white\")\n",
        "    )\n",
        "\n",
        "    # --- Forecast Table ---\n",
        "    filtered_df = df[df[\"county_name\"] == county_name]\n",
        "    columns = [{\"name\": i, \"id\": i} for i in filtered_df.columns]\n",
        "    data = filtered_df.to_dict(\"records\")\n",
        "\n",
        "    # --- Compute County Change (2015–2019) ---\n",
        "    df_change = (\n",
        "        df[df[\"year\"].between(2015, 2019)]\n",
        "        .groupby(\"county_name\")[\"Linear_Forecast\"]\n",
        "        .agg([\"first\", \"last\"])\n",
        "        .reset_index()\n",
        "    )\n",
        "    df_change[\"Forest_Cover_Change_2015_2019\"] = df_change[\"last\"] - df_change[\"first\"]\n",
        "\n",
        "    # Thresholds for categorization\n",
        "    change_column = df_change[\"Forest_Cover_Change_2015_2019\"]\n",
        "    loss_threshold = change_column.quantile(0.25)\n",
        "    gain_threshold = change_column.quantile(0.75)\n",
        "\n",
        "    def categorize_change(change):\n",
        "        if change < loss_threshold:\n",
        "            return \"Significant Loss\"\n",
        "        elif change > gain_threshold:\n",
        "            return \"Gain\"\n",
        "        else:\n",
        "            return \"Slight Loss/Stable\"\n",
        "\n",
        "    df_change[\"Canopy_Change_Category\"] = df_change[\"Forest_Cover_Change_2015_2019\"].apply(categorize_change)\n",
        "\n",
        "    # --- EDA Clustering Plot (category-based) ---\n",
        "    eda_fig = px.scatter(\n",
        "        df_change,\n",
        "        x=\"first\", y=\"last\",\n",
        "        color=\"Canopy_Change_Category\",\n",
        "        hover_name=\"county_name\",\n",
        "        title=f\"Clustering of Counties by Canopy Change (2015–2019) - {state_name}\",\n",
        "        labels={\"first\": \"2015 Cover (%)\", \"last\": \"2019 Cover (%)\"}\n",
        "    )\n",
        "    eda_fig.update_layout(plot_bgcolor=\"#111111\", paper_bgcolor=\"#111111\", font=dict(color=\"white\"))\n",
        "\n",
        "    # --- Countywise Change Plot (increase/decrease) ---\n",
        "    df_sorted = df_change.sort_values(\"Forest_Cover_Change_2015_2019\", ascending=False)\n",
        "    countywise_fig = px.bar(\n",
        "        df_sorted,\n",
        "        x=\"Forest_Cover_Change_2015_2019\", y=\"county_name\",\n",
        "        color=\"Canopy_Change_Category\",\n",
        "        title=f\"Forest Cover Change (2015–2019) by County - {state_name}\",\n",
        "        orientation=\"h\"\n",
        "    )\n",
        "    countywise_fig.update_layout(\n",
        "        plot_bgcolor=\"#111111\", paper_bgcolor=\"#111111\", font=dict(color=\"white\"),\n",
        "        yaxis={'categoryorder':'total ascending'}\n",
        "    )\n",
        "\n",
        "    return fig_forecast, columns, data, eda_fig, countywise_fig\n",
        "\n",
        "# --- Run inside Colab/Jupyter ---\n",
        "app.run(mode=\"inline\")\n"
      ],
      "metadata": {
        "id": "yMEFhGOxEvW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datacommons_pandas as dcpd\n",
        "import pandas as pd\n",
        "\n",
        "# --- Helper: get counties for a state ---\n",
        "def get_counties(state_dcid):\n",
        "    counties = dcpd.get_places_in([state_dcid], \"County\")\n",
        "    return dict(zip(counties[\"place\"], counties[\"name\"]))\n",
        "\n",
        "# --- Build \"forecast-like\" dataframe from Data Commons ---\n",
        "def get_state_forecast_from_dc(state_dcid, state_name):\n",
        "    counties = get_counties(state_dcid)\n",
        "    frames = []\n",
        "\n",
        "    for county_dcid, county_name in counties.items():\n",
        "        # Fetch tree canopy % timeseries\n",
        "        df = dcpd.build_time_series(county_dcid, \"PercentArea_TreeCanopy\")\n",
        "        if df.empty:\n",
        "            continue\n",
        "        df = df.reset_index().rename(columns={\"index\": \"year\"})\n",
        "\n",
        "        # Standardize columns to match your Dash/Streamlit functions\n",
        "        df = df.rename(columns={\"PercentArea_TreeCanopy\": \"Linear_Forecast\"})\n",
        "        df[\"ARIMA_Forecast\"] = None  # placeholder for compatibility\n",
        "        df[\"county_name\"] = county_name\n",
        "        df[\"state_name\"] = state_name\n",
        "        frames.append(df)\n",
        "\n",
        "    if not frames:\n",
        "        return pd.DataFrame(columns=[\"year\",\"Linear_Forecast\",\"ARIMA_Forecast\",\"county_name\",\"state_name\"])\n",
        "\n",
        "    return pd.concat(frames, ignore_index=True)"
      ],
      "metadata": {
        "id": "Rg3ImiF2o3w_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio datacommons datacommons_pandas plotly -q\n",
        "\n"
      ],
      "metadata": {
        "id": "TLhpuipgjAUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import json\n",
        "import urllib.request\n",
        "\n",
        "# --- Reuse your existing functions ---\n",
        "# get_state_forest_dataframe(), analyze_state(), state_geoids dict\n",
        "\n",
        "# --- Load US counties GeoJSON dynamically ---\n",
        "url = \"https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json\"\n",
        "with urllib.request.urlopen(url) as response:\n",
        "    counties_geojson = json.load(response)\n",
        "\n",
        "# --- State-level dashboard ---\n",
        "def state_dashboard(state_name):\n",
        "    dcid = state_geoids[state_name]\n",
        "    df_long = get_state_forest_dataframe(dcid)\n",
        "    if df_long.empty:\n",
        "        return \"No data\", None, None, None  # added map as 4th output\n",
        "\n",
        "    forecast_df, _ = analyze_state(df_long, geoID=dcid.split(\"/\")[-1], horizon=10)\n",
        "\n",
        "    df_change = (\n",
        "        forecast_df[forecast_df[\"year\"].between(2015, 2019)]\n",
        "        .groupby([\"Fips\", \"county_name\"])[\"Linear_Forecast\"]\n",
        "        .agg([\"first\", \"last\"]).reset_index()\n",
        "    )\n",
        "    df_change[\"Change\"] = df_change[\"last\"] - df_change[\"first\"]\n",
        "    df_change[\"Category\"] = pd.cut(\n",
        "        df_change[\"Change\"],\n",
        "        bins=[-999, -1, 1, 999],\n",
        "        labels=[\"Loss\", \"Stable\", \"Gain\"]\n",
        "    )\n",
        "\n",
        "    # Scatter plot\n",
        "    eda_fig = px.scatter(\n",
        "        df_change, x=\"first\", y=\"last\", color=\"Category\",\n",
        "        hover_name=\"county_name\",\n",
        "        title=f\"{state_name} Canopy Change Clustering\"\n",
        "    )\n",
        "\n",
        "    # Bar plot\n",
        "    bar_fig = px.bar(\n",
        "        df_change, x=\"Change\", y=\"county_name\",\n",
        "        color=\"Category\",\n",
        "        color_discrete_map={\"Loss\":\"crimson\", \"Stable\":\"dimgray\", \"Gain\":\"forestgreen\"},\n",
        "        title=f\"{state_name} Forest Cover Change (2015–2019)\", orientation=\"h\"\n",
        "    )\n",
        "\n",
        "    # Scrollable table\n",
        "    html_table = (\n",
        "        \"<div style='max-height:400px; overflow-y:auto; border:1px solid #ccc'>\"\n",
        "        + df_change.to_html(index=False)\n",
        "        + \"</div>\"\n",
        "    )\n",
        "\n",
        "    # --- US Choropleth Map ---\n",
        "    fig_map = px.choropleth(\n",
        "        df_change,\n",
        "        geojson=counties_geojson,\n",
        "        locations='Fips',\n",
        "        color='Change',\n",
        "        color_continuous_scale='RdYlGn',\n",
        "        scope=\"usa\",\n",
        "        hover_name='county_name',\n",
        "        title=f\"{state_name} County Canopy Change (2015-2019)\"\n",
        "    )\n",
        "    fig_map.update_geos(fitbounds=\"locations\", visible=False)\n",
        "\n",
        "    return html_table, eda_fig, bar_fig, fig_map\n",
        "\n",
        "# --- County-level dashboard ---\n",
        "def county_dashboard(state_name, county_name):\n",
        "    dcid = state_geoids[state_name]\n",
        "    df_long = get_state_forest_dataframe(dcid)\n",
        "    if df_long.empty:\n",
        "        return \"No data\", None\n",
        "\n",
        "    forecast_df, _ = analyze_state(df_long, geoID=dcid.split(\"/\")[-1], horizon=10)\n",
        "\n",
        "    if county_name not in forecast_df[\"county_name\"].unique():\n",
        "        county_name = forecast_df[\"county_name\"].iloc[0]\n",
        "\n",
        "    county_df = forecast_df[forecast_df[\"county_name\"] == county_name]\n",
        "\n",
        "    fig_forecast = px.line(county_df, x=\"year\", y=\"Linear_Forecast\",\n",
        "                           title=f\"{county_name}, {state_name} Forecast\")\n",
        "\n",
        "    return county_df.to_html(index=False), fig_forecast\n",
        "\n",
        "def get_counties(state_name):\n",
        "    dcid = state_geoids[state_name]\n",
        "    df_long = get_state_forest_dataframe(dcid)\n",
        "    return list(df_long[\"county_name\"].unique())\n",
        "\n",
        "# --- Gradio UI ---\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# 🌲 Forest Cover Dashboard (State vs County)\")\n",
        "\n",
        "    # --- State tab ---\n",
        "    with gr.Tab(\"State View\"):\n",
        "        st_state = gr.Dropdown(choices=list(state_geoids.keys()), value=\"Georgia\", label=\"Select State\")\n",
        "        st_btn = gr.Button(\"Show State Results\")\n",
        "        st_table = gr.HTML()\n",
        "        st_scatter = gr.Plot()\n",
        "        st_bar = gr.Plot()\n",
        "        st_map = gr.Plot()  # new map\n",
        "        st_btn.click(state_dashboard, inputs=[st_state], outputs=[st_table, st_scatter, st_bar, st_map])\n",
        "\n",
        "    # --- County tab (exactly your original code) ---\n",
        "    with gr.Tab(\"County View\"):\n",
        "        ct_state = gr.Dropdown(choices=list(state_geoids.keys()), value=\"Georgia\", label=\"Select State\")\n",
        "        ct_county = gr.Dropdown(choices=[], label=\"Select County\")\n",
        "        ct_btn = gr.Button(\"Show County Forecast\")\n",
        "        ct_table = gr.HTML()\n",
        "        ct_plot = gr.Plot()\n",
        "\n",
        "        # update county list dynamically\n",
        "        def update_county_choices(state_name):\n",
        "            counties = get_counties(state_name)\n",
        "            return gr.Dropdown(choices=counties, value=counties[0] if counties else None)\n",
        "\n",
        "        ct_state.change(update_county_choices, inputs=ct_state, outputs=ct_county)\n",
        "        ct_btn.click(county_dashboard, inputs=[ct_state, ct_county], outputs=[ct_table, ct_plot])\n",
        "\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "id": "coBPJ7sbjI0X"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}